/*! For license information please see chunk.2d929f406cb86343ba9b.js.LICENSE.txt */
"use strict";(self.webpackChunkai_workout_assistant=self.webpackChunkai_workout_assistant||[]).push([[228],{15841:(t,e,i)=>{i.d(e,{EH:()=>a,Qp:()=>r,bu:()=>n,l7:()=>s,pf:()=>l});class s extends Error{constructor(t){super(t),Object.setPrototypeOf(this,s.prototype)}}class n extends Error{constructor(t){super(t),Object.setPrototypeOf(this,n.prototype)}}class r extends Error{constructor(t){super(t),Object.setPrototypeOf(this,r.prototype)}}class a extends Error{constructor(t){super(t),Object.setPrototypeOf(this,a.prototype)}}class l extends Error{constructor(t){super(t),Object.setPrototypeOf(this,l.prototype)}}Error},98918:(t,e,i)=>{i.d(e,{O0:()=>m});var s=i(36115),n=i(45978);(0,s._K2)().registerFlag("TOPOLOGICAL_SORT_CACHE_MAX_ENTRIES",(()=>100),n.oR),i(71888);var r=i(87504),a=i(1303),l=i(78825),o=i(79730),h=(i(85244),i(66079)),u=i(91686),c=i(15841),p=i(55795),d=i(44813),g=i(91928),f=i(63057);async function m(t,e){if(null==e&&(e={}),"string"==typeof t){const i=s.io.getLoadHandlers(t,e);if(0===i.length)i.push(s.io.browserHTTPRequest(t,e));else if(i.length>1)throw new c.Qp(`Found more than one (${i.length}) load handlers for URL '${t}'`);t=i[0]}return async function(t,e,i){if(null==i&&(i={}),null==t.load)throw new c.Qp("Cannot proceed with model loading because the IOHandler provided does not have the `load` method implemented.");const n=await t.load();let r=n.modelTopology;null!=r.model_config&&(r=r.model_config);const a=null==i.strict||i.strict,l=null!=n.weightData&&null!=n.weightSpecs&&a,o=(0,p.i)((0,g.w)(r),void 0,l),h=n.trainingConfig;if(null!=h&&o.loadTrainingConfig(h),null!=n.userDefinedMetadata&&o.setUserDefinedMetadata(n.userDefinedMetadata),null!=n.weightData){if(null==n.weightSpecs)throw new c.Qp("LayersModel artifacts contains weight data, but not weight specs. Therefore loading of weights cannot proceed.");const{modelWeights:t,optimizerWeights:e}=function(t,e){const i=s.io.decodeWeights(t,e),n={},r=[];return e.forEach((t=>{"optimizer"===t.group?r.push({name:t.name,tensor:i[t.name]}):n[t.name]=i[t.name]})),{modelWeights:n,optimizerWeights:r}}(n.weightData,n.weightSpecs);o.loadWeights(t,a),null!=o.optimizer&&e.length>0&&await o.optimizer.setWeights(e),(0,s.ASo)(t),(0,s.ASo)(e.map((t=>t.tensor)))}return o}(t,0,e)}class b extends h.Gw{constructor(t){if(super({inputs:[],outputs:[]}),t=t||{},this.trainable=!0,this.built=!1,this.name=null!=t.name?t.name:(0,u.v)("sequential_"),null!=t.layers)for(const e of t.layers)this.add(e)}checkShape(t){if(t.inboundNodes[0].outputTensors[0].shape.some((t=>t<0)))throw new c.Qp(`Negative dimension size caused by adding layer ${t.name} with input shape [${t.inboundNodes[0].inputTensors[0].shape}]`)}add(t){const e=t instanceof b||t instanceof h.Gw;let i;if(e){if(i=t,1!==i.outputs.length)throw new c.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");if(1!==i.inputs.length)throw new c.Qp("All layers in a Sequential model should have a single input tensor. For multi-input layers, use the functional API.")}if(0===this.outputs.length){if(0===t.inboundNodes.length){if(null==t.batchInputShape)throw new c.Qp("The first layer in a Sequential model must get an `inputShape` or `batchInputShape` argument.");const e=(0,l.p)({batchShape:t.batchInputShape,dtype:t.dtype,name:t.name+"_input"});t.apply(e)}if(e)this.outputs=i.outputs,this.inputs=i.inputs;else{if(1!==t.inboundNodes.length)throw new c.Qp(`A layer added to a Sequential model must not already be connected somewhere else. LayersModel received layer ${t.name} which has ${t.inboundNodes.length} pre-existing inbound connections.`);if(1!==t.inboundNodes[0].outputTensors.length)throw new c.Qp("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[t.inboundNodes[0].outputTensors[0]],this.inputs=(0,o.X6)(this.outputs[0])}this.inboundNodes=[],new o.bP({outboundLayer:this,inboundLayers:[],nodeIndices:[],tensorIndices:[],inputTensors:this.inputs,outputTensors:this.outputs,inputMasks:d.fD(null,this.inputs.length),outputMasks:[null],inputShapes:this.inputs.map((t=>t.shape)),outputShapes:this.outputs[0].shape})}else{const e=t.apply(this.outputs[0]);if(Array.isArray(e))throw new TypeError("All layers in a Sequential model should have a single output tensor. For multi-output layers, use the functional API.");this.checkShape(t),this.outputs=[e],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}this.layers.push(t),this.built=!1}pop(){if(0===this.layers.length)throw new TypeError("There are no layers in the model.");if(this.layers.pop(),0===this.layers.length)this.outputs=[],this.inboundNodes=[],this.outboundNodes=[];else{const t=this.layers.length-1;this.layers[t].outboundNodes=[],this.outputs=[this.layers[t].output],this.inboundNodes[0].outputTensors=this.outputs,this.inboundNodes[0].outputShapes=[this.outputs[0].shape]}}call(t,e){return null==this.model&&this.build(),this.model.call(t,e)}build(t){if((0,f.U$)(t),0===this.inputs.length||0===this.outputs.length)throw new TypeError("Sequential model cannot be built: model is empty. Add some layers first.");this.model=new h.Gw({inputs:this.inputs,outputs:this.outputs[0],name:this.name+"_model"}),this.model.trainable=this.trainable,this.supportsMasking=this.model.supportsMasking,this.inputLayers=this.model.inputLayers,this.inputLayersNodeIndices=this.model.inputLayersNodeIndices,this.inputLayersTensorIndices=this.model.inputLayersTensorIndices,this.outputLayers=this.model.outputLayers,this.outputLayersNodeIndices=this.model.outputLayersNodeIndices,this.outputLayersTensorIndices=this.model.outputLayersTensorIndices,this.nodesByDepth=this.model.nodesByDepth,this.containerNodes=this.model.containerNodes,this.outputNames=this.model.outputNames,this.inputNames=this.model.inputNames,this.built=!0}countParams(){return this.built||this.build(),super.countParams()}summary(t,e,i=console.log){this.built||this.build(),super.summary(t,e,i)}setWeights(t){null==this.model&&this.build(),this.model.setWeights(t)}evaluate(t,e,i={}){if(!this.built)throw new c.bu("The model needs to be compiled before being used.");return this.model.evaluate(t,e,i)}async evaluateDataset(t,e){if(!this.built)throw new c.bu("The model needs to be compiled before being used.");return this.model.evaluateDataset(t,e)}predict(t,e={}){return null==this.model&&this.build(),this.model.predict(t,e)}predictOnBatch(t){return null==this.model&&this.build(),this.model.predictOnBatch(t)}compile(t){this.build(),this.model.compile(t),this.optimizer_=this.model.optimizer,this.isOptimizerOwned=this.model.isOptimizerOwned,this.loss=this.model.loss,this.metrics=this.model.metrics,this.metricsTensors=this.model.metricsTensors,this.metricsNames=this.model.metricsNames}get optimizer(){return null==this.model?void 0:this.model.optimizer}set optimizer(t){this.model.optimizer=t}async fit(t,e,i={}){if(!this.built)throw new c.bu("The model needs to be compiled before being used.");return this.model.fit(t,e,i)}async fitDataset(t,e){if(!this.built)throw new c.bu("The model needs to be compiled before being used.");return this.model.fitDataset(t,e)}async trainOnBatch(t,e){return this.model.trainOnBatch(t,e)}static fromConfig(t,e,i={},n=!1){let r,a={};if(e instanceof Array){if(null==e[0].className||"Merge"===e[0].className)throw new c.Qp("Legacy serialization format not supported yet.");r=e}else s.ZSL.assert(null!=e.layers,(()=>"When the config data for a Sequential model is not an Array, it must be an Object that contains the 'layers' field.")),r=e.layers,delete e.layers,a=e;const l=new t(a);if(!(l instanceof b))throw new c.EH(`Sequential.fromConfig called on non-Sequential input: ${l}`);for(const t of r){const e=void 0,i=(0,p.i)(t,e,n);n&&i.setFastWeightInitDuringBuild(!0),l.add(i)}return l}set stopTraining(t){if(null==this.model)throw new c.Qp("Cannot set the stopTraining property of a sequential model before it is compiled.");this.model.stopTraining=t}get stopTraining(){if(null==this.model)throw new c.Qp("Cannot get the stopTraining property of a sequential model before it is compiled.");return this.model.stopTraining}getConfig(){const t=[];for(const e of this.layers){const i={};i.className=e.getClassName(),i.config=e.getConfig(),t.push(i)}return{name:this.name,layers:t}}}b.className="Sequential",s.JFn.registerClass(b);var y=i(59885),w=i(47661);class z extends s.JFn.Serializable{}class S extends z{constructor(t){super(),function(t){if(null!=t&&"object"!=typeof t)throw new Error(`Argument to L1L2 regularizer's constructor is expected to be an object, but received: ${t}`)}(t),this.l1=null==t||null==t.l1?.01:t.l1,this.l2=null==t||null==t.l2?.01:t.l2,this.hasL1=0!==this.l1,this.hasL2=0!==this.l2}apply(t){return(0,s.DZQ)((()=>{let e=(0,s.Ul9)([1]);return this.hasL1&&(e=(0,s.WQq)(e,(0,s.czq)(s.lKK(this.l1,(0,s.tnl)(t))))),this.hasL2&&(e=(0,s.WQq)(e,(0,s.czq)(s.lKK(this.l2,w.Ew(t))))),s.tQQ(e,[])}))}getConfig(){return{l1:this.l1,l2:this.l2}}static fromConfig(t,e){return new t({l1:e.l1,l2:e.l2})}}S.className="L1L2",s.JFn.registerClass(S);const k={l1l2:"L1L2"};function C(t){return(0,d.k4)(t)}function A(t,e={}){return(0,d.Xv)(t,s.JFn.SerializationMap.getMap().classNameMap,e,"regularizer")}function v(t){return null==t?null:"string"==typeof t?A({className:t in k?k[t]:t,config:{}}):t instanceof z?t:A(t)}class I extends o.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,null!=t&&(this.maxValue=t.maxValue)}call(t,e){t=(0,f.un)(t);let i=(0,s.VVh)(t);return null!=this.maxValue&&(i=(0,s.zQh)(i,0,this.maxValue)),i}computeOutputShape(t){return t}getConfig(){const t={maxValue:this.maxValue},e=super.getConfig();return Object.assign(t,e),t}}I.className="ReLU",s.JFn.registerClass(I);class N extends o.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_ALPHA=.3,null==t&&(t={}),this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const i=(0,f.un)(t);return(0,s.H8d)(i,this.alpha)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}N.className="LeakyReLU",s.JFn.registerClass(N);class F extends o.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA_INITIALIZER="zeros",null==t&&(t={}),this.supportsMasking=!0,this.alphaInitializer=(0,a.Fe)(t.alphaInitializer||this.DEFAULT_ALPHA_INITIALIZER),this.alphaRegularizer=v(t.alphaRegularizer),this.alphaConstraint=(0,r.YZ)(t.alphaConstraint),null==t.sharedAxes)this.sharedAxes=null;else if(Array.isArray(t.sharedAxes))this.sharedAxes=t.sharedAxes;else{if("number"!=typeof t.sharedAxes)throw new c.Qp(`Expected sharedAxes to be a number or an array of numbers, but got ${t.sharedAxes}`);this.sharedAxes=[t.sharedAxes]}}build(t){const e=(t=(0,f.U$)(t)).slice(1);if(null!=this.sharedAxes)for(const t of this.sharedAxes)e[t-1]=1;this.alpha=this.addWeight("alpha",e,"float32",this.alphaInitializer,this.alphaRegularizer,!0,this.alphaConstraint);const i={};if(null!=this.sharedAxes)for(let e=1;e<t.length;++e)i[e]=t[e];this.inputSpec=[new o.eO({ndim:t.length,axes:i})],this.built=!0}call(t,e){return t=(0,f.un)(t),(0,s.NsG)(t,this.alpha.read())}getConfig(){const t={alphaInitializer:(0,a.zo)(this.alphaInitializer),alphaRegularizer:C(this.alphaRegularizer),alphaConstraint:(0,r.uH)(this.alphaConstraint),sharedAxes:this.sharedAxes},e=super.getConfig();return Object.assign(t,e),t}}F.className="PReLU",s.JFn.registerClass(F);class D extends o.Wd{constructor(t){if(super(null==t?{}:t),this.DEFAULT_ALPHA=1,null==t&&(t={}),null!=t.alpha&&t.alpha!==this.DEFAULT_ALPHA)throw new c.EH(`Non-default alpha value (${t.alpha}) is not supported by the ELU layer yet.`);this.alpha=null==t.alpha?this.DEFAULT_ALPHA:t.alpha}call(t,e){const i=(0,f.un)(t);return(0,s.Pqc)(i)}computeOutputShape(t){return t}getConfig(){const t={alpha:this.alpha},e=super.getConfig();return Object.assign(t,e),t}}D.className="ELU",s.JFn.registerClass(D);class R extends o.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_THETA=1,null==t&&(t={}),this.theta=null==t.theta?this.DEFAULT_THETA:t.theta}call(t,e){const i=(0,f.un)(t);return(0,s.lKK)(i,(0,s.wgE)((0,s.rhj)(i,this.theta),"float32"))}computeOutputShape(t){return t}getConfig(){const t={theta:this.theta},e=super.getConfig();return Object.assign(t,e),t}}R.className="ThresholdedReLU",s.JFn.registerClass(R);class E extends o.Wd{constructor(t){super(null==t?{}:t),this.DEFAULT_AXIS=1,null==t&&(t={}),this.softmax=(new y.rF).apply,this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis}call(t,e){const i=(0,f.un)(t);return this.softmax(i,this.axis)}computeOutputShape(t){return t}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}E.className="Softmax",s.JFn.registerClass(E);var x=i(6090),Q=i(39459),L=i(73072);function T(t,e,i){if("number"==typeof t)return(0,d.fD)(t,e);if(t.length!==e)throw new c.Qp(`The ${i} argument must be an integer or tuple of ${e} integers. Received: ${t.length} elements.`);for(let s=0;s<e;++s){const n=t[s];if(!(0,L.Fq)(n))throw new c.Qp(`The ${i} argument must be an integer or tuple of ${e} integers. Received: ${JSON.stringify(t)} including a non-integer number ${n}`)}return t}function O(t,e,i,s,n=1){if(null==t)return t;let r;return r="same"===i?t:t-(e+(e-1)*(n-1))+1,Math.floor((r+s-1)/s)}function M(t,e,i,s){if(null==t)return null;if("valid"===s)t=t*e+(0,L.T9)([i-e,0]);else{if("same"!==s)throw new c.Qp(`Unsupport padding mode: ${s}.`);t*=e}return t}function $(t,e){return(0,s.DZQ)((()=>((0,Q.uM)(e),"channelsFirst"===e?s.mgz(t,[0,2,3,1]):t)))}function _(t,e){return(0,s.DZQ)((()=>((0,Q.uM)(e),"channelsFirst"===e?s.mgz(t,[0,2,3,4,1]):t)))}function U(t,e,i,n=[1,1],r="valid",a,l,o=null){return(0,s.DZQ)((()=>{if(null==a&&(a=(0,x.VI)()),(0,Q.uM)(a),3!==t.rank&&4!==t.rank)throw new c.Qp(`conv2dWithBiasActivation expects input to be of rank 3 or 4, but received ${t.rank}.`);if(3!==e.rank&&4!==e.rank)throw new c.Qp(`conv2dWithBiasActivation expects kernel to be of rank 3 or 4, but received ${t.rank}.`);let h=$(t,a);if("causal"===r)throw new c.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");return h=s.cZk.conv2d({x:h,filter:e,strides:n,pad:"same"===r?"same":"valid",dilations:l,dataFormat:"NHWC",bias:i,activation:o}),"channelsFirst"===a&&(h=s.mgz(h,[0,3,1,2])),h}))}class Z extends o.Wd{constructor(t,e){if(super(e),this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",Z.verifyArgs(e),this.rank=t,d.oo(this.rank,"rank"),1!==this.rank&&2!==this.rank&&3!==this.rank)throw new c.EH(`Convolution layer for rank other than 1, 2, or 3 (${this.rank}) is not implemented yet.`);if(this.kernelSize=T(e.kernelSize,t,"kernelSize"),this.strides=T(null==e.strides?1:e.strides,t,"strides"),this.padding=null==e.padding?"valid":e.padding,(0,Q.tB)(this.padding),this.dataFormat=null==e.dataFormat?"channelsLast":e.dataFormat,(0,Q.uM)(this.dataFormat),this.activation=(0,y.b_)(e.activation),this.useBias=null==e.useBias||e.useBias,this.biasInitializer=(0,a.Fe)(e.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.biasConstraint=(0,r.YZ)(e.biasConstraint),this.biasRegularizer=v(e.biasRegularizer),this.activityRegularizer=v(e.activityRegularizer),this.dilationRate=T(null==e.dilationRate?1:e.dilationRate,t,"dilationRate"),1===this.rank&&Array.isArray(this.dilationRate)&&1!==this.dilationRate.length)throw new c.Qp(`dilationRate must be a number or an array of a single number for 1D convolution, but received ${JSON.stringify(this.dilationRate)}`);if(2===this.rank){if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate];else if(2!==this.dilationRate.length)throw new c.Qp(`dilationRate must be a number or array of two numbers for 2D convolution, but received ${JSON.stringify(this.dilationRate)}`)}else if(3===this.rank)if("number"==typeof this.dilationRate)this.dilationRate=[this.dilationRate,this.dilationRate,this.dilationRate];else if(3!==this.dilationRate.length)throw new c.Qp(`dilationRate must be a number or array of three numbers for 3D convolution, but received ${JSON.stringify(this.dilationRate)}`)}static verifyArgs(t){if(d.vA("kernelSize"in t,"required key 'kernelSize' not in config"),"number"!=typeof t.kernelSize&&!d.HP(t.kernelSize,"number",1,3))throw new c.Qp(`BaseConv expects config.kernelSize to be number or number[] with length 1, 2, or 3, but received ${JSON.stringify(t.kernelSize)}.`)}getConfig(){const t={kernelSize:this.kernelSize,strides:this.strides,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,activation:(0,y.Bu)(this.activation),useBias:this.useBias,biasInitializer:(0,a.zo)(this.biasInitializer),biasRegularizer:C(this.biasRegularizer),activityRegularizer:C(this.activityRegularizer),biasConstraint:(0,r.uH)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}class W extends Z{constructor(t,e){super(t,e),this.kernel=null,W.verifyArgs(e),this.filters=e.filters,d.oo(this.filters,"filters"),this.kernelInitializer=(0,a.Fe)(e.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.kernelConstraint=(0,r.YZ)(e.kernelConstraint),this.kernelRegularizer=v(e.kernelRegularizer)}build(t){t=(0,f.U$)(t);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.Qp(`The channel dimension of the input should be defined. Found ${t[e]}`);const i=t[e],s=this.kernelSize.concat([i,this.filters]);this.kernel=this.addWeight("kernel",s,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[{ndim:this.rank+2,axes:{[e]:i}}],this.built=!0}call(t,e){return(0,s.DZQ)((()=>{let e;t=(0,f.un)(t);const i=null==this.bias?null:this.bias.read(),n=d.Cd(this.activation.getClassName());if(null!=n&&2===this.rank)e=U(t,this.kernel.read(),i,this.strides,this.padding,this.dataFormat,this.dilationRate,n);else{if(1===this.rank)e=function(t,e,i,n=1,r="valid",a,l=1){return(0,s.DZQ)((()=>{if(null==a&&(a=(0,x.VI)()),(0,Q.uM)(a),3!==t.shape.length)throw new c.Qp(`The input of a conv1dWithBias operation should be 3, but is ${t.shape.length} instead.`);if(3!==e.shape.length)throw new c.Qp(`The kernel for a conv1dWithBias operation should be 3, but is ${e.shape.length} instead`);if(null!=i&&1!==i.shape.length)throw new c.Qp(`The bias for a conv1dWithBias operation should be 1, but is ${e.shape.length} instead`);if("channelsFirst"===a&&(t=s.mgz(t,[0,2,1])),"causal"===r)throw new c.EH("The support for CAUSAL padding mode in conv1dWithBias is not implemented yet.");let o=s.kA9(t,e,n,"same"===r?"same":"valid","NWC",l);return null!=i&&(o=w.ni(o,i)),o}))}(t,this.kernel.read(),i,this.strides[0],this.padding,this.dataFormat,this.dilationRate[0]);else if(2===this.rank)e=U(t,this.kernel.read(),i,this.strides,this.padding,this.dataFormat,this.dilationRate);else{if(3!==this.rank)throw new c.EH("convolutions greater than 3D are not implemented yet.");e=function(t,e,i,n=[1,1,1],r="valid",a,l){return(0,s.DZQ)((()=>{if(null==a&&(a=(0,x.VI)()),(0,Q.uM)(a),4!==t.rank&&5!==t.rank)throw new c.Qp(`conv3dWithBias expects input to be of rank 4 or 5, but received ${t.rank}.`);if(4!==e.rank&&5!==e.rank)throw new c.Qp(`conv3dWithBias expects kernel to be of rank 4 or 5, but received ${t.rank}.`);let o=_(t,a);if("causal"===r)throw new c.EH("The support for CAUSAL padding mode in conv3dWithBias is not implemented yet.");return o=s.IPL(o,e,n,"same"===r?"same":"valid","NDHWC",l),null!=i&&(o=w.ni(o,i)),"channelsFirst"===a&&(o=s.mgz(o,[0,4,1,2,3])),o}))}(t,this.kernel.read(),i,this.strides,this.padding,this.dataFormat,this.dilationRate)}null!=this.activation&&(e=this.activation.apply(e))}return e}))}computeOutputShape(t){t=(0,f.U$)(t);const e=[],i="channelsLast"===this.dataFormat?t.slice(1,t.length-1):t.slice(2);for(let t=0;t<i.length;++t){const s=O(i[t],this.kernelSize[t],this.padding,this.strides[t],"number"==typeof this.dilationRate?this.dilationRate:this.dilationRate[t]);e.push(s)}let s=[t[0]];return"channelsLast"===this.dataFormat?(s=s.concat(e),s.push(this.filters)):(s.push(this.filters),s=s.concat(e)),s}getConfig(){const t={filters:this.filters,kernelInitializer:(0,a.zo)(this.kernelInitializer),kernelRegularizer:C(this.kernelRegularizer),kernelConstraint:(0,r.uH)(this.kernelConstraint)},e=super.getConfig();return Object.assign(t,e),t}static verifyArgs(t){if(!("filters"in t)||"number"!=typeof t.filters||t.filters<1)throw new c.Qp(`Convolution layer expected config.filters to be a 'number' > 0 but got ${JSON.stringify(t.filters)}`)}}class B extends W{constructor(t){super(2,t),B.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!d.HP(t.kernelSize,"number",1,2))throw new c.Qp(`Conv2D expects config.kernelSize to be number or number[] with length 1 or 2, but received ${JSON.stringify(t.kernelSize)}.`)}}B.className="Conv2D",s.JFn.registerClass(B);class K extends W{constructor(t){super(3,t),K.verifyArgs(t)}getConfig(){const t=super.getConfig();return delete t.rank,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&(!Array.isArray(t.kernelSize)||1!==t.kernelSize.length&&3!==t.kernelSize.length))throw new c.Qp(`Conv3D expects config.kernelSize to be number or [number, number, number], but received ${JSON.stringify(t.kernelSize)}.`)}}K.className="Conv3D",s.JFn.registerClass(K);class j extends B{constructor(t){if(super(t),this.inputSpec=[new o.eO({ndim:4})],"same"!==this.padding&&"valid"!==this.padding)throw new c.Qp(`Conv2DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(4!==(t=(0,f.U$)(t)).length)throw new c.Qp("Input should have rank 4; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.Qp("The channel dimension of the inputs should be defined. Found `None`.");const i=t[e],s=this.kernelSize.concat([this.filters,i]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new o.eO({ndim:4,axes:{[e]:i}})],this.built=!0}call(t,e){return s.DZQ((()=>{let e=(0,f.un)(t);if(4!==e.shape.length)throw new c.Qp(`Conv2DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const i=e.shape,n=i[0];let r,a;"channelsFirst"===this.dataFormat?(r=2,a=3):(r=1,a=2);const l=i[r],o=i[a],h=this.kernelSize[0],u=this.kernelSize[1],p=this.strides[0],d=this.strides[1],g=[n,M(l,p,h,this.padding),M(o,d,u,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=s.mgz(e,[0,2,3,1]));let m=s.wX9(e,this.kernel.read(),g,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(m=s.mgz(m,[0,3,1,2])),null!=this.bias&&(m=w.ni(m,this.bias.read(),this.dataFormat)),null!=this.activation&&(m=this.activation.apply(m)),m}))}computeOutputShape(t){const e=(t=(0,f.U$)(t)).slice();let i,s,n;"channelsFirst"===this.dataFormat?(i=1,s=2,n=3):(i=3,s=1,n=2);const r=this.kernelSize[0],a=this.kernelSize[1],l=this.strides[0],o=this.strides[1];return e[i]=this.filters,e[s]=M(e[s],l,r,this.padding),e[n]=M(e[n],o,a,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}j.className="Conv2DTranspose",s.JFn.registerClass(j);class J extends K{constructor(t){if(super(t),this.inputSpec=[new o.eO({ndim:5})],"same"!==this.padding&&"valid"!==this.padding)throw new c.Qp(`Conv3DTranspose currently supports only padding modes 'same' and 'valid', but received padding mode ${this.padding}`)}build(t){if(5!==(t=(0,f.U$)(t)).length)throw new c.Qp("Input should have rank 5; Received input shape: "+JSON.stringify(t));const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e])throw new c.Qp("The channel dimension of the inputs should be defined. Found `None`.");const i=t[e],s=this.kernelSize.concat([this.filters,i]);this.kernel=this.addWeight("kernel",s,"float32",this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint)),this.inputSpec=[new o.eO({ndim:5,axes:{[e]:i}})],this.built=!0}call(t,e){return s.DZQ((()=>{let e=(0,f.un)(t);if(5!==e.shape.length)throw new c.Qp(`Conv3DTranspose.call() expects input tensor to be rank-4, but received a tensor of rank-${e.shape.length}`);const i=e.shape,n=i[0];let r,a,l;"channelsFirst"===this.dataFormat?(l=2,r=3,a=4):(l=1,r=2,a=3);const o=i[l],h=i[r],u=i[a],p=this.kernelSize[0],d=this.kernelSize[1],g=this.kernelSize[2],m=this.strides[0],b=this.strides[1],y=this.strides[2],z=[n,M(o,m,p,this.padding),M(h,b,d,this.padding),M(u,y,g,this.padding),this.filters];"channelsLast"!==this.dataFormat&&(e=s.mgz(e,[0,2,3,4,1]));let S=s.jIJ(e,this.kernel.read(),z,this.strides,this.padding);return"channelsLast"!==this.dataFormat&&(S=s.mgz(S,[0,4,1,2,3])),null!==this.bias&&(S=w.ni(S,this.bias.read(),this.dataFormat)),null!==this.activation&&(S=this.activation.apply(S)),S}))}computeOutputShape(t){const e=(t=(0,f.U$)(t)).slice();let i,s,n,r;"channelsFirst"===this.dataFormat?(i=1,s=2,n=3,r=4):(i=4,s=1,n=2,r=3);const a=this.kernelSize[0],l=this.kernelSize[1],o=this.kernelSize[2],h=this.strides[0],u=this.strides[1],c=this.strides[2];return e[i]=this.filters,e[s]=M(e[s],h,a,this.padding),e[n]=M(e[n],u,l,this.padding),e[r]=M(e[r],c,o,this.padding),e}getConfig(){const t=super.getConfig();return delete t.dilationRate,t}}J.className="Conv3DTranspose",s.JFn.registerClass(J);class q extends W{constructor(t,e){if(super(t,e),this.DEFAULT_DEPTHWISE_INITIALIZER="glorotUniform",this.DEFAULT_POINTWISE_INITIALIZER="glorotUniform",this.depthwiseKernel=null,this.pointwiseKernel=null,null==e.filters)throw new c.Qp("The `filters` configuration field is required by SeparableConv, but is unspecified.");if(null!=e.kernelInitializer||null!=e.kernelRegularizer||null!=e.kernelConstraint)throw new c.Qp("Fields kernelInitializer, kernelRegularizer and kernelConstraint are invalid for SeparableConv2D. Use depthwiseInitializer, depthwiseRegularizer, depthwiseConstraint, pointwiseInitializer, pointwiseRegularizer and pointwiseConstraint instead.");if(null!=e.padding&&"same"!==e.padding&&"valid"!==e.padding)throw new c.Qp(`SeparableConv${this.rank}D supports only padding modes: 'same' and 'valid', but received ${JSON.stringify(e.padding)}`);this.depthMultiplier=null==e.depthMultiplier?1:e.depthMultiplier,this.depthwiseInitializer=(0,a.Fe)(e.depthwiseInitializer||this.DEFAULT_DEPTHWISE_INITIALIZER),this.depthwiseRegularizer=v(e.depthwiseRegularizer),this.depthwiseConstraint=(0,r.YZ)(e.depthwiseConstraint),this.pointwiseInitializer=(0,a.Fe)(e.depthwiseInitializer||this.DEFAULT_POINTWISE_INITIALIZER),this.pointwiseRegularizer=v(e.pointwiseRegularizer),this.pointwiseConstraint=(0,r.YZ)(e.pointwiseConstraint)}build(t){if((t=(0,f.U$)(t)).length<this.rank+2)throw new c.Qp(`Inputs to SeparableConv${this.rank}D should have rank ${this.rank+2}, but received input shape: ${JSON.stringify(t)}`);const e="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[e]||t[e]<0)throw new c.Qp(`The channel dimension of the inputs should be defined, but found ${JSON.stringify(t[e])}`);const i=t[e],s=this.kernelSize.concat([i,this.depthMultiplier]),n=[];for(let t=0;t<this.rank;++t)n.push(1);n.push(i*this.depthMultiplier,this.filters);const r=!0;this.depthwiseKernel=this.addWeight("depthwise_kernel",s,"float32",this.depthwiseInitializer,this.depthwiseRegularizer,r,this.depthwiseConstraint),this.pointwiseKernel=this.addWeight("pointwise_kernel",n,"float32",this.pointwiseInitializer,this.pointwiseRegularizer,r,this.pointwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[this.filters],"float32",this.biasInitializer,this.biasRegularizer,r,this.biasConstraint):this.bias=null,this.inputSpec=[new o.eO({ndim:this.rank+2,axes:{[e]:i}})],this.built=!0}call(t,e){return(0,s.DZQ)((()=>{let e;if(t=(0,f.un)(t),1===this.rank)throw new c.EH("1D separable convolution is not implemented yet.");return 2===this.rank&&("channelsFirst"===this.dataFormat&&(t=s.mgz(t,[0,2,3,1])),e=s.wdz(t,this.depthwiseKernel.read(),this.pointwiseKernel.read(),this.strides,this.padding,this.dilationRate,"NHWC")),this.useBias&&(e=w.ni(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),"channelsFirst"===this.dataFormat&&(e=s.mgz(e,[0,3,1,2])),e}))}getConfig(){const t=super.getConfig();return delete t.rank,delete t.kernelInitializer,delete t.kernelRegularizer,delete t.kernelConstraint,t.depthwiseInitializer=(0,a.zo)(this.depthwiseInitializer),t.pointwiseInitializer=(0,a.zo)(this.pointwiseInitializer),t.depthwiseRegularizer=C(this.depthwiseRegularizer),t.pointwiseRegularizer=C(this.pointwiseRegularizer),t.depthwiseConstraint=(0,r.uH)(this.depthwiseConstraint),t.pointwiseConstraint=(0,r.uH)(this.pointwiseConstraint),t}}q.className="SeparableConv";class H extends q{constructor(t){super(2,t)}}H.className="SeparableConv2D",s.JFn.registerClass(H);class P extends W{constructor(t){super(1,t),P.verifyArgs(t),this.inputSpec=[{ndim:3}]}getConfig(){const t=super.getConfig();return delete t.rank,delete t.dataFormat,t}static verifyArgs(t){if("number"!=typeof t.kernelSize&&!d.HP(t.kernelSize,"number",1,1))throw new c.Qp(`Conv1D expects config.kernelSize to be number or number[] with length 1, but received ${JSON.stringify(t.kernelSize)}.`)}}P.className="Conv1D",s.JFn.registerClass(P);class V extends o.Wd{constructor(t){super(t),"number"==typeof t.cropping?this.cropping=[[t.cropping,t.cropping],[t.cropping,t.cropping]]:"number"==typeof t.cropping[0]?this.cropping=[[t.cropping[0],t.cropping[0]],[t.cropping[1],t.cropping[1]]]:this.cropping=t.cropping,this.dataFormat=void 0===t.dataFormat?"channelsLast":t.dataFormat,this.inputSpec=[{ndim:4}]}computeOutputShape(t){return"channelsFirst"===this.dataFormat?[t[0],t[1],t[2]-this.cropping[0][0]-this.cropping[0][1],t[3]-this.cropping[1][0]-this.cropping[1][1]]:[t[0],t[1]-this.cropping[0][0]-this.cropping[0][1],t[2]-this.cropping[1][0]-this.cropping[1][1],t[3]]}call(t,e){return(0,s.DZQ)((()=>{if(t=(0,f.un)(t),"channelsLast"===this.dataFormat){const e=w.r0(t,this.cropping[0][0],t.shape[1]-this.cropping[0][0]-this.cropping[0][1],2);return w.r0(e,this.cropping[1][0],t.shape[2]-this.cropping[1][1]-this.cropping[1][0],3)}{const e=w.r0(t,this.cropping[0][0],t.shape[2]-this.cropping[0][0]-this.cropping[0][1],3);return w.r0(e,this.cropping[1][0],t.shape[3]-this.cropping[1][1]-this.cropping[1][0],4)}}))}getConfig(){const t={cropping:this.cropping,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}V.className="Cropping2D",s.JFn.registerClass(V);class G extends o.Wd{constructor(t){super(t),this.DEFAULT_SIZE=[2,2],this.inputSpec=[{ndim:4}],this.size=null==t.size?this.DEFAULT_SIZE:t.size,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,Q.uM)(this.dataFormat),this.interpolation=null==t.interpolation?"nearest":t.interpolation,(0,Q.uU)(this.interpolation)}computeOutputShape(t){if("channelsFirst"===this.dataFormat){const e=null==t[2]?null:this.size[0]*t[2],i=null==t[3]?null:this.size[1]*t[3];return[t[0],t[1],e,i]}{const e=null==t[1]?null:this.size[0]*t[1],i=null==t[2]?null:this.size[1]*t[2];return[t[0],e,i,t[3]]}}call(t,e){return s.DZQ((()=>{let e=(0,f.un)(t);const i=e.shape;if("channelsFirst"===this.dataFormat){e=s.mgz(e,[0,2,3,1]);const t=this.size[0]*i[2],n=this.size[1]*i[3],r="nearest"===this.interpolation?s.image.resizeNearestNeighbor(e,[t,n]):s.image.resizeBilinear(e,[t,n]);return s.mgz(r,[0,3,1,2])}{const t=this.size[0]*i[1],n=this.size[1]*i[2];return"nearest"===this.interpolation?s.image.resizeNearestNeighbor(e,[t,n]):s.image.resizeBilinear(e,[t,n])}}))}getConfig(){const t={size:this.size,dataFormat:this.dataFormat,interpolation:this.interpolation},e=super.getConfig();return Object.assign(t,e),t}}G.className="UpSampling2D",s.JFn.registerClass(G);class Y extends Z{constructor(t){super(2,t),this.depthwiseKernel=null,this.depthMultiplier=null==t.depthMultiplier?1:t.depthMultiplier,this.depthwiseInitializer=(0,a.Fe)(t.depthwiseInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.depthwiseConstraint=(0,r.YZ)(t.depthwiseConstraint),this.depthwiseRegularizer=v(t.depthwiseRegularizer)}build(t){if((t=(0,f.U$)(t)).length<4)throw new c.Qp(`Inputs to DepthwiseConv2D should have rank 4. Received input shape: ${JSON.stringify(t)}.`);const e="channelsFirst"===this.dataFormat?1:3;if(null==t[e]||t[e]<0)throw new c.Qp(`The channel dimension of the inputs to DepthwiseConv2D should be defined, but is not (${t[e]}).`);const i=t[e],s=[this.kernelSize[0],this.kernelSize[1],i,this.depthMultiplier];this.depthwiseKernel=this.addWeight("depthwise_kernel",s,null,this.depthwiseInitializer,this.depthwiseRegularizer,!0,this.depthwiseConstraint),this.useBias?this.bias=this.addWeight("bias",[i*this.depthMultiplier],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,s.DZQ)((()=>{let e=function(t,e,i=[1,1],n="valid",r,a){return(0,s.DZQ)((()=>{null==r&&(r=(0,x.VI)()),(0,Q.uM)(r);let l=$(t,r);if(4!==t.rank)throw new c.Qp(`Input for depthwiseConv2d is required to be 4-D, but is instead ${t.rank}-D`);if(4!==e.rank)throw new c.Qp(`depthwiseKernel is required to be 4-D, but is instead ${e.rank}-D`);return l=s.Gl3(l,e,i,"same"===n?"same":"valid","NHWC",a),"channelsFirst"===r&&(l=s.mgz(l,[0,3,1,2])),l}))}(t=(0,f.un)(t),this.depthwiseKernel.read(),this.strides,this.padding,this.dataFormat,null);return this.useBias&&(e=w.ni(e,this.bias.read(),this.dataFormat)),null!=this.activation&&(e=this.activation.apply(e)),e}))}computeOutputShape(t){t=(0,f.U$)(t);const e="channelsFirst"===this.dataFormat?t[2]:t[1],i="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[1]*this.depthMultiplier:t[3]*this.depthMultiplier,n=O(e,this.kernelSize[0],this.padding,this.strides[0]),r=O(i,this.kernelSize[1],this.padding,this.strides[1]);return"channelsFirst"===this.dataFormat?[t[0],s,n,r]:[t[0],n,r,s]}getConfig(){const t=super.getConfig();return t.depthMultiplier=this.depthMultiplier,t.depthwiseInitializer=(0,a.zo)(this.depthwiseInitializer),t.depthwiseRegularizer=C(this.depthwiseRegularizer),t.depthwiseConstraint=(0,r.uH)(this.depthwiseRegularizer),t}}Y.className="DepthwiseConv2D",s.JFn.registerClass(Y);var X=i(71765);function tt(t,e,i,s){if(Array.isArray(t)){if(null!=e||null!=i)throw new c.Qp("When inputs is an array, neither initialState or constants should be provided");null!=s&&(i=t.slice(t.length-s,t.length),t=t.slice(0,t.length-s)),t.length>1&&(e=t.slice(1,t.length)),t=t[0]}function n(t){return null==t||Array.isArray(t)?t:[t]}return{inputs:t,initialState:e=n(e),constants:i=n(i)}}function et(t,e,i,n=!1,r,a,l=!1,o=!1){return s.DZQ((()=>{const h=e.shape.length;if(h<3)throw new c.Qp(`Input should be at least 3D, but is ${h}D.`);const u=[1,0].concat(L.y1(2,h));if(e=s.mgz(e,u),null!=a)throw new c.EH("The rnn() functoin of the deeplearn.js backend does not support constants yet.");l&&console.warn("Backend rnn(): the unroll = true option is not applicable to the imperative deeplearn.js backend."),null!=r&&((r=s.wgE(s.wgE(r,"bool"),"float32")).rank===h-1&&(r=s.UG6(r,-1)),r=s.mgz(r,u)),n&&(e=s.BEg(e,0),null!=r&&(r=s.BEg(r,0)));const p=[];let d,g=i;const f=e.shape[0],m=s.K$i(e);let b,y;null!=r&&(b=s.K$i(r));for(let e=0;e<f;++e){const i=m[e],n=s.DZQ((()=>t(i,g)));if(null==r)d=n[0],g=n[1];else{const t=s.DZQ((()=>{const t=b[e],i=s.jbE(s.P61(t),t),r=s.WQq(s.lKK(n[0],t),s.lKK(g[0],i)),a=g.map(((e,r)=>s.WQq(s.lKK(n[1][r],t),s.lKK(e,i))));return{output:r,newStates:a}}));d=t.output,g=t.newStates}o&&p.push(d)}if(o){const t=1;y=s.t$z(p,t)}return[d,y,g]}))}class it extends o.Wd{constructor(t){let e;if(super(t),null==t.cell)throw new c.Qp("cell property is missing for the constructor of RNN.");if(e=Array.isArray(t.cell)?new ut({cells:t.cell}):t.cell,null==e.stateSize)throw new c.Qp("The RNN cell should have an attribute `stateSize` (tuple of integers, one integer per RNN state).");this.cell=e,this.returnSequences=null!=t.returnSequences&&t.returnSequences,this.returnState=null!=t.returnState&&t.returnState,this.goBackwards=null!=t.goBackwards&&t.goBackwards,this._stateful=null!=t.stateful&&t.stateful,this.unroll=null!=t.unroll&&t.unroll,this.supportsMasking=!0,this.inputSpec=[new o.eO({ndim:3})],this.stateSpec=null,this.states_=null,this.numConstants=null,this.keptStates=[]}getStates(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;return L.y1(0,t).map((t=>null))}return this.states_}setStates(t){this.states_=t}computeOutputShape(t){(0,f.TT)(t)&&(t=t[0]);let e=this.cell.stateSize;Array.isArray(e)||(e=[e]);const i=e[0];let s;if(s=this.returnSequences?[t[0],t[1],i]:[t[0],i],this.returnState){const i=[];for(const s of e)i.push([t[0],s]);return[s].concat(i)}return s}computeMask(t,e){return s.DZQ((()=>{Array.isArray(e)&&(e=e[0]);const t=this.returnSequences?e:null;if(this.returnState){const e=this.states.map((t=>null));return[t].concat(e)}return t}))}get states(){if(null==this.states_){const t=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1,e=[];for(let i=0;i<t;++i)e.push(null);return e}return this.states_}set states(t){this.states_=t}build(t){if(null!=this.numConstants)throw new c.EH("Constants support is not implemented in RNN yet.");(0,f.TT)(t)&&(t=t[0]);const e=this.stateful?t[0]:null,i=t.slice(2);this.inputSpec[0]=new o.eO({shape:[e,null,...i]});const n=[t[0]].concat(t.slice(2));let r;if(this.cell.build(n),r=Array.isArray(this.cell.stateSize)?this.cell.stateSize:[this.cell.stateSize],null!=this.stateSpec){if(!s.ZSL.arraysEqual(this.stateSpec.map((t=>t.shape[t.shape.length-1])),r))throw new c.Qp(`An initialState was passed that is not compatible with cell.stateSize. Received stateSpec=${this.stateSpec}; However cell.stateSize is ${this.cell.stateSize}`)}else this.stateSpec=r.map((t=>new o.eO({shape:[null,t]})));this.stateful&&this.resetStates()}resetStates(t,e=!1){(0,s.DZQ)((()=>{if(!this.stateful)throw new c.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const i=this.inputSpec[0].shape[0];if(null==i)throw new c.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.states_)Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>s.Ul9([i,t]))):this.states_=[s.Ul9([i,this.cell.stateSize])];else if(null==t)s.ASo(this.states_),null!=this.keptStates&&(s.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((t=>s.Ul9([i,t]))):this.states_[0]=s.Ul9([i,this.cell.stateSize]);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new c.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);!0===e?this.keptStates.push(this.states_.slice()):s.ASo(this.states_);for(let e=0;e<this.states_.length;++e){const n=t[e],r=Array.isArray(this.cell.stateSize)?this.cell.stateSize[e]:this.cell.stateSize,a=[i,r];if(!s.ZSL.arraysEqual(n.shape,a))throw new c.Qp(`State ${e} is incompatible with layer ${this.name}: expected shape=${a}, received shape=${n.shape}`);this.states_[e]=n}}this.states_=this.states_.map((t=>s.aCs(t.clone())))}))}apply(t,e){let i=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const n=tt(t,i,s,this.numConstants);t=n.inputs,i=n.initialState,s=n.constants;let r=[],a=[];if(null!=i){e.initialState=i,r=r.concat(i),this.stateSpec=[];for(const t of i)this.stateSpec.push(new o.eO({shape:t.shape}));a=a.concat(this.stateSpec)}if(null!=s&&(e.constants=s,r=r.concat(s),this.numConstants=s.length),r[0]instanceof o.Ar){const i=[t].concat(r),s=this.inputSpec.concat(a),n=this.inputSpec;this.inputSpec=s;const l=super.apply(i,e);return this.inputSpec=n,l}return super.apply(t,e)}call(t,e){return(0,s.DZQ)((()=>{const i=null==e?null:e.mask,s=null==e?null:e.training;let n=null==e?null:e.initialState;t=(0,f.un)(t),null==n&&(n=this.stateful?this.states_:this.getInitialState(t));const r=Array.isArray(this.cell.stateSize)?this.cell.stateSize.length:1;if(n.length!==r)throw new c.Qp(`RNN Layer has ${r} state(s) but was passed ${n.length} initial state(s).`);this.unroll&&console.warn("Ignoring unroll = true for RNN layer, due to imperative backend.");const a={training:s},l=et(((t,e)=>{const i=this.cell.call([t].concat(e),a);return[i[0],i.slice(1)]}),t,n,this.goBackwards,i,null,this.unroll,this.returnSequences),o=l[0],h=l[1],u=l[2];this.stateful&&this.resetStates(u,s);const p=this.returnSequences?h:o;return this.returnState?[p].concat(u):p}))}getInitialState(t){return(0,s.DZQ)((()=>{let e=s.Ul9(t.shape);return e=s.czq(e,[1,2]),e=w.UG(e),Array.isArray(this.cell.stateSize)?this.cell.stateSize.map((t=>t>1?w.Vs(e,[1,t]):e)):this.cell.stateSize>1?[w.Vs(e,[1,this.cell.stateSize])]:[e]}))}get trainableWeights(){return this.trainable?this.cell.trainableWeights:[]}get nonTrainableWeights(){return this.trainable?this.cell.nonTrainableWeights:this.cell.weights}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.cell&&this.cell.setFastWeightInitDuringBuild(t)}getConfig(){const t=super.getConfig(),e={returnSequences:this.returnSequences,returnState:this.returnState,goBackwards:this.goBackwards,stateful:this.stateful,unroll:this.unroll};null!=this.numConstants&&(e.numConstants=this.numConstants);const i=this.cell.getConfig();return this.getClassName()===it.className&&(e.cell={className:this.cell.getClassName(),config:i}),Object.assign(Object.assign(Object.assign({},i),t),e)}static fromConfig(t,e,i={}){const s=e.cell,n=(0,p.i)(s,i);return new t(Object.assign(e,{cell:n}))}}it.className="RNN",s.JFn.registerClass(it);class st extends o.Wd{}class nt extends st{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,d.oo)(this.units,"units"),this.activation=(0,y.b_)(null==t.activation?this.DEFAULT_ACTIVATION:t.activation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=v(t.kernelRegularizer),this.recurrentRegularizer=v(t.recurrentRegularizer),this.biasRegularizer=v(t.biasRegularizer),this.kernelConstraint=(0,r.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,r.YZ)(t.recurrentConstraint),this.biasConstraint=(0,r.YZ)(t.biasConstraint),this.dropout=L.jk([1,L.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=L.jk([1,L.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){t=(0,f.U$)(t),this.kernel=this.addWeight("kernel",[t[t.length-1],this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,s.DZQ)((()=>{if(2!==t.length)throw new c.Qp(`SimpleRNNCell expects 2 input Tensors, got ${t.length}.`);let i=t[1];t=t[0];const n=null!=e.training&&e.training;let r;0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>s.P61(t),rate:this.dropout,training:n,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>s.P61(i),rate:this.recurrentDropout,training:n,dropoutFunc:this.dropoutFunc}));const a=this.dropoutMask,l=this.recurrentDropoutMask;r=null!=a?w.Om(s.lKK(t,a),this.kernel.read()):w.Om(t,this.kernel.read()),null!=this.bias&&(r=w.ni(r,this.bias.read())),null!=l&&(i=s.lKK(i,l));let o=s.WQq(r,w.Om(i,this.recurrentKernel.read()));return null!=this.activation&&(o=this.activation.apply(o)),[o,o]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,a.zo)(this.kernelInitializer),recurrentInitializer:(0,a.zo)(this.recurrentInitializer),biasInitializer:(0,a.zo)(this.biasInitializer),kernelRegularizer:C(this.kernelRegularizer),recurrentRegularizer:C(this.recurrentRegularizer),biasRegularizer:C(this.biasRegularizer),activityRegularizer:C(this.activityRegularizer),kernelConstraint:(0,r.uH)(this.kernelConstraint),recurrentConstraint:(0,r.uH)(this.recurrentConstraint),biasConstraint:(0,r.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout};return Object.assign(Object.assign({},t),e)}}nt.className="SimpleRNNCell",s.JFn.registerClass(nt);class rt extends it{constructor(t){t.cell=new nt(t),super(t)}call(t,e){return(0,s.DZQ)((()=>{null!=this.cell.dropoutMask&&(s.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(s.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const i=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:i,training:n,initialState:r})}))}static fromConfig(t,e){return new t(e)}}rt.className="SimpleRNN",s.JFn.registerClass(rt);class at extends st{constructor(t){if(super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",t.resetAfter)throw new c.Qp("GRUCell does not support reset_after parameter set to true.");this.units=t.units,(0,d.oo)(this.units,"units"),this.activation=(0,y.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,y.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelRegularizer=v(t.kernelRegularizer),this.recurrentRegularizer=v(t.recurrentRegularizer),this.biasRegularizer=v(t.biasRegularizer),this.kernelConstraint=(0,r.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,r.YZ)(t.recurrentConstraint),this.biasConstraint=(0,r.YZ)(t.biasConstraint),this.dropout=L.jk([1,L.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=L.jk([1,L.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=this.units,this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){const e=(t=(0,f.U$)(t))[t.length-1];this.kernel=this.addWeight("kernel",[e,3*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,3*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias?this.bias=this.addWeight("bias",[3*this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint):this.bias=null,this.built=!0}call(t,e){return(0,s.DZQ)((()=>{if(2!==t.length)throw new c.Qp(`GRUCell expects 2 input Tensors (inputs, h, c), got ${t.length}.`);const i=null!=e.training&&e.training;let n=t[1];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>s.P61(t),rate:this.dropout,training:i,count:3,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>s.P61(n),rate:this.recurrentDropout,training:i,count:3,dropoutFunc:this.dropoutFunc}));const r=this.dropoutMask,a=this.recurrentDropoutMask;let l,o,h;0<this.dropout&&this.dropout<1&&(t=s.lKK(t,r[0]));let u=w.Om(t,this.kernel.read());this.useBias&&(u=w.ni(u,this.bias.read())),0<this.recurrentDropout&&this.recurrentDropout<1&&(n=s.lKK(n,a[0]));const p=this.recurrentKernel.read(),[d,g]=s.lDo(p,[2*this.units,this.units],p.rank-1),f=w.Om(n,d),[m,b,y]=s.lDo(u,3,u.rank-1),[z,S]=s.lDo(f,2,f.rank-1);l=this.recurrentActivation.apply(s.WQq(m,z)),o=this.recurrentActivation.apply(s.WQq(b,S));const k=w.Om(s.lKK(o,n),g);h=this.activation.apply(s.WQq(y,k));const C=s.WQq(s.lKK(l,n),s.lKK(s.WQq(1,s.HZy(l)),h));return[C,C]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.Bu)(this.activation),recurrentActivation:(0,y.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,a.zo)(this.kernelInitializer),recurrentInitializer:(0,a.zo)(this.recurrentInitializer),biasInitializer:(0,a.zo)(this.biasInitializer),kernelRegularizer:C(this.kernelRegularizer),recurrentRegularizer:C(this.recurrentRegularizer),biasRegularizer:C(this.biasRegularizer),activityRegularizer:C(this.activityRegularizer),kernelConstraint:(0,r.uH)(this.kernelConstraint),recurrentConstraint:(0,r.uH)(this.recurrentConstraint),biasConstraint:(0,r.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation,resetAfter:!1};return Object.assign(Object.assign({},t),e)}}at.className="GRUCell",s.JFn.registerClass(at);class lt extends it{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new at(t),super(t)}call(t,e){return(0,s.DZQ)((()=>{null!=this.cell.dropoutMask&&(s.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(s.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const i=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:i,training:n,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}lt.className="GRU",s.JFn.registerClass(lt);class ot extends st{constructor(t){super(t),this.DEFAULT_ACTIVATION="tanh",this.DEFAULT_RECURRENT_ACTIVATION="hardSigmoid",this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_RECURRENT_INITIALIZER="orthogonal",this.DEFAULT_BIAS_INITIALIZER="zeros",this.units=t.units,(0,d.oo)(this.units,"units"),this.activation=(0,y.b_)(void 0===t.activation?this.DEFAULT_ACTIVATION:t.activation),this.recurrentActivation=(0,y.b_)(void 0===t.recurrentActivation?this.DEFAULT_RECURRENT_ACTIVATION:t.recurrentActivation),this.useBias=null==t.useBias||t.useBias,this.kernelInitializer=(0,a.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.recurrentInitializer=(0,a.Fe)(t.recurrentInitializer||this.DEFAULT_RECURRENT_INITIALIZER),this.biasInitializer=(0,a.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.unitForgetBias=t.unitForgetBias,this.kernelRegularizer=v(t.kernelRegularizer),this.recurrentRegularizer=v(t.recurrentRegularizer),this.biasRegularizer=v(t.biasRegularizer),this.kernelConstraint=(0,r.YZ)(t.kernelConstraint),this.recurrentConstraint=(0,r.YZ)(t.recurrentConstraint),this.biasConstraint=(0,r.YZ)(t.biasConstraint),this.dropout=L.jk([1,L.T9([0,null==t.dropout?0:t.dropout])]),this.recurrentDropout=L.jk([1,L.T9([0,null==t.recurrentDropout?0:t.recurrentDropout])]),this.dropoutFunc=t.dropoutFunc,this.implementation=t.implementation,this.stateSize=[this.units,this.units],this.dropoutMask=null,this.recurrentDropoutMask=null}build(t){var e;const i=(t=(0,f.U$)(t))[t.length-1];let s;if(this.kernel=this.addWeight("kernel",[i,4*this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.recurrentKernel=this.addWeight("recurrent_kernel",[this.units,4*this.units],null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){if(this.unitForgetBias){const t=this.biasInitializer,i=this.units;s=new((e=class extends a.H4{apply(e,s){const n=t.apply([i]),r=(new a.sN).apply([i]),l=t.apply([2*i]);return w.ly(w.ly(n,r),l)}}).className="CustomInit",e)}else s=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.units],null,s,this.biasRegularizer,!0,this.biasConstraint)}else this.bias=null;this.built=!0}call(t,e){return(0,s.DZQ)((()=>{const i=null!=e.training&&e.training;if(3!==t.length)throw new c.Qp(`LSTMCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);let n=t[1];const r=t[2];t=t[0],0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>s.P61(t),rate:this.dropout,training:i,count:4,dropoutFunc:this.dropoutFunc})),0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>s.P61(n),rate:this.recurrentDropout,training:i,count:4,dropoutFunc:this.dropoutFunc}));const a=this.dropoutMask,l=this.recurrentDropoutMask;let o,h,u,p;0<this.dropout&&this.dropout<1&&(t=s.lKK(t,a[0]));let d=w.Om(t,this.kernel.read());0<this.recurrentDropout&&this.recurrentDropout<1&&(n=s.lKK(n,l[0])),d=s.WQq(d,w.Om(n,this.recurrentKernel.read())),this.useBias&&(d=w.ni(d,this.bias.read()));const[g,f,m,b]=s.lDo(d,4,d.rank-1);o=this.recurrentActivation.apply(g),h=this.recurrentActivation.apply(f),u=s.WQq(s.lKK(h,r),s.lKK(o,this.activation.apply(m))),p=this.recurrentActivation.apply(b);const y=s.lKK(p,this.activation.apply(u));return[y,y,u]}))}getConfig(){const t=super.getConfig(),e={units:this.units,activation:(0,y.Bu)(this.activation),recurrentActivation:(0,y.Bu)(this.recurrentActivation),useBias:this.useBias,kernelInitializer:(0,a.zo)(this.kernelInitializer),recurrentInitializer:(0,a.zo)(this.recurrentInitializer),biasInitializer:(0,a.zo)(this.biasInitializer),unitForgetBias:this.unitForgetBias,kernelRegularizer:C(this.kernelRegularizer),recurrentRegularizer:C(this.recurrentRegularizer),biasRegularizer:C(this.biasRegularizer),activityRegularizer:C(this.activityRegularizer),kernelConstraint:(0,r.uH)(this.kernelConstraint),recurrentConstraint:(0,r.uH)(this.recurrentConstraint),biasConstraint:(0,r.uH)(this.biasConstraint),dropout:this.dropout,recurrentDropout:this.recurrentDropout,implementation:this.implementation};return Object.assign(Object.assign({},t),e)}}ot.className="LSTMCell",s.JFn.registerClass(ot);class ht extends it{constructor(t){0===t.implementation&&console.warn("`implementation=0` has been deprecated, and now defaults to `implementation=1`. Please update your layer call."),t.cell=new ot(t),super(t)}call(t,e){return(0,s.DZQ)((()=>{null!=this.cell.dropoutMask&&(s.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(s.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null);const i=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:i,training:n,initialState:r})}))}static fromConfig(t,e){return 0===e.implmentation&&(e.implementation=1),new t(e)}}ht.className="LSTM",s.JFn.registerClass(ht);class ut extends st{constructor(t){super(t),this.cells=t.cells}get stateSize(){const t=[];for(const e of this.cells.slice().reverse())Array.isArray(e.stateSize)?t.push(...e.stateSize):t.push(e.stateSize);return t}call(t,e){return(0,s.DZQ)((()=>{let i=t.slice(1);const s=[];for(const t of this.cells.slice().reverse())Array.isArray(t.stateSize)?s.push(i.splice(0,t.stateSize.length)):s.push(i.splice(0,1));s.reverse();const n=[];let r;for(let a=0;a<this.cells.length;++a){const l=this.cells[a];i=s[a],r=0===a?[t[0]].concat(i):[r[0]].concat(i),r=l.call(r,e),n.push(r.slice(1))}i=[];for(const t of n.slice().reverse())i.push(...t);return[r[0]].concat(i)}))}build(t){let e;(0,f.TT)(t)&&(t=t[0]),this.cells.forEach(((i,s)=>{(0,Q.IU)(`RNNCell_${s}`,(()=>{i.build(t),e=Array.isArray(i.stateSize)?i.stateSize[0]:i.stateSize,t=[t[0],e]}))})),this.built=!0}getConfig(){const t=super.getConfig(),e={cells:this.cells.map((t=>({className:t.getClassName(),config:t.getConfig()})))};return Object.assign(Object.assign({},t),e)}static fromConfig(t,e,i={}){const s=[];for(const t of e.cells)s.push((0,p.i)(t,i));return new t({cells:s})}get trainableWeights(){if(!this.trainable)return[];const t=[];for(const e of this.cells)t.push(...e.trainableWeights);return t}get nonTrainableWeights(){const t=[];for(const e of this.cells)t.push(...e.nonTrainableWeights);if(!this.trainable){const e=[];for(const t of this.cells)e.push(...t.trainableWeights);return e.concat(t)}return t}getWeights(){const t=[];for(const e of this.cells)t.push(...e.weights);return(0,X.ex)(t)}setWeights(t){const e=[];for(const i of this.cells){const s=i.weights.length,n=t.splice(s);for(let t=0;t<i.weights.length;++t)e.push([i.weights[t],n[t]])}(0,X.UM)(e)}}function ct(t){const{ones:e,rate:i,training:n=!1,count:r=1,dropoutFunc:a}=t,l=()=>null!=a?a(e(),i):w.EZ(e(),i),o=()=>w.Ls(l,e,n);return!r||r<=1?s.aCs(o().clone()):Array(r).fill(void 0).map(o).map((t=>s.aCs(t.clone())))}ut.className="StackedRNNCells",s.JFn.registerClass(ut);class pt extends it{constructor(t){if(t.unroll)throw new c.EH("Unrolling is not possible with convolutional RNNs.");if(Array.isArray(t.cell))throw new c.EH("It is not possible at the moment to stack convolutional cells.");super(t),this.inputSpec=[new o.eO({ndim:5})]}call(t,e){return s.DZQ((()=>{if(null!=this.cell.dropoutMask&&(s.ASo(this.cell.dropoutMask),this.cell.dropoutMask=null),null!=this.cell.recurrentDropoutMask&&(s.ASo(this.cell.recurrentDropoutMask),this.cell.recurrentDropoutMask=null),e&&e.constants)throw new c.Qp("ConvRNN2D cell does not support constants");const i=null==e?null:e.mask,n=null==e?null:e.training,r=null==e?null:e.initialState;return super.call(t,{mask:i,training:n,initialState:r})}))}computeOutputShape(t){let e=this.computeSingleOutputShape(t);return this.returnSequences||(e=[e[0],...e.slice(2)]),this.returnState&&(e=[e,...Array(2).fill([t[0],...e.slice(-3)])]),e}getInitialState(t){return s.DZQ((()=>{const{stateSize:e}=this.cell,i=t.shape,n=this.computeSingleOutputShape(i),r=[n[0],...n.slice(2)],a=s.Ul9(r);return Array.isArray(e)?Array(e.length).fill(a):[a]}))}resetStates(t,e=!1){s.DZQ((()=>{if(!this.stateful)throw new c.l7("Cannot call resetStates() on an RNN Layer that is not stateful.");const i=this.inputSpec[0].shape,n=this.computeSingleOutputShape(i),r=[n[0],...n.slice(2)];if(null==i[0])throw new c.Qp("If an RNN is stateful, it needs to know its batch size. Specify the batch size of your input tensors: \n- If using a Sequential model, specify the batch size by passing a `batchInputShape` option to your first layer.\n- If using the functional API, specify the batch size by passing a `batchShape` option to your Input layer.");if(null==this.getStates())Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>s.Ul9(r))):this.states_=[s.Ul9(r)];else if(null==t)s.ASo(this.states_),null!=this.keptStates&&(s.ASo(this.keptStates),this.keptStates=[]),Array.isArray(this.cell.stateSize)?this.states_=this.cell.stateSize.map((()=>s.Ul9(r))):this.states_[0]=s.Ul9(r);else{if(Array.isArray(t)||(t=[t]),t.length!==this.states_.length)throw new c.Qp(`Layer ${this.name} expects ${this.states_.length} state(s), but it received ${t.length} state value(s). Input received: ${t}`);e?this.keptStates.push(this.states_.slice()):s.ASo(this.states_);for(let e=0;e<this.states_.length;++e){const i=t[e],n=r;if(!s.ZSL.arraysEqual(i.shape,n))throw new c.Qp(`State ${e} is incompatible with layer ${this.name}: expected shape=${n}, received shape=${i.shape}`);this.states_[e]=i}}this.states_=this.states_.map((t=>s.aCs(t.clone())))}))}computeSingleOutputShape(t){const{dataFormat:e,filters:i,kernelSize:s,padding:n,strides:r,dilationRate:a}=this.cell,l="channelsFirst"===e,o=t[l?3:2],h=t[l?4:3],u=O(o,s[0],n,r[0],a[0]),c=O(h,s[1],n,r[1],a[1]);return[...t.slice(0,2),...l?[i,u,c]:[u,c,i]]}}pt.className="ConvRNN2D";class dt extends ot{constructor(t){const{filters:e,kernelSize:i,strides:s,padding:n,dataFormat:r,dilationRate:a}=t;super(Object.assign(Object.assign({},t),{units:e})),this.filters=e,(0,d.oo)(this.filters,"filters"),this.kernelSize=T(i,2,"kernelSize"),this.kernelSize.forEach((t=>(0,d.oo)(t,"kernelSize"))),this.strides=T(s||1,2,"strides"),this.strides.forEach((t=>(0,d.oo)(t,"strides"))),this.padding=n||"valid",(0,Q.tB)(this.padding),this.dataFormat=r||"channelsLast",(0,Q.uM)(this.dataFormat),this.dilationRate=T(a||1,2,"dilationRate"),this.dilationRate.forEach((t=>(0,d.oo)(t,"dilationRate")))}build(t){var e;t=(0,f.U$)(t);const i="channelsFirst"===this.dataFormat?1:t.length-1;if(null==t[i])throw new c.Qp(`The channel dimension of the input should be defined. Found ${t[i]}`);const n=t[i],r=this.kernelSize.concat([n,4*this.filters]);this.kernel=this.addWeight("kernel",r,null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint);const l=this.kernelSize.concat([this.filters,4*this.filters]);if(this.recurrentKernel=this.addWeight("recurrent_kernel",l,null,this.recurrentInitializer,this.recurrentRegularizer,!0,this.recurrentConstraint),this.useBias){let t;if(this.unitForgetBias){const i=this.biasInitializer,n=this.filters;t=new((e=class extends a.H4{apply(t,e){const r=i.apply([n]),a=s.SaS([n]),l=i.apply([2*n]);return w.u1([r,a,l])}}).className="CustomInit",e)}else t=this.biasInitializer;this.bias=this.addWeight("bias",[4*this.filters],null,t,this.biasRegularizer,!0,this.biasConstraint)}this.built=!0}call(t,e){return s.DZQ((()=>{if(3!==t.length)throw new c.Qp(`ConvLSTM2DCell expects 3 input Tensors (inputs, h, c), got ${t.length}.`);const i=e.training||!1,n=t[0],r=t[1],a=t[2];0<this.dropout&&this.dropout<1&&null==this.dropoutMask&&(this.dropoutMask=ct({ones:()=>s.P61(n),rate:this.dropout,training:i,count:4,dropoutFunc:this.dropoutFunc}));const l=this.dropoutMask,o=(t,e,i)=>e&&e[i]?s.lKK(e[i],t):t;let h=o(n,l,0),u=o(n,l,1),p=o(n,l,2),d=o(n,l,3);0<this.recurrentDropout&&this.recurrentDropout<1&&null==this.recurrentDropoutMask&&(this.recurrentDropoutMask=ct({ones:()=>s.P61(r),rate:this.recurrentDropout,training:i,count:4,dropoutFunc:this.dropoutFunc}));const g=this.recurrentDropoutMask;let f=o(r,g,0),m=o(r,g,1),b=o(r,g,2),y=o(r,g,3);const[w,z,S,k]=s.lDo(this.kernel.read(),4,3),[C,A,v,I]=this.useBias?s.lDo(this.bias.read(),4):[null,null,null,null];h=this.inputConv(h,w,C,this.padding),u=this.inputConv(u,z,A,this.padding),p=this.inputConv(p,S,v,this.padding),d=this.inputConv(d,k,I,this.padding);const[N,F,D,R]=s.lDo(this.recurrentKernel.read(),4,3);f=this.recurrentConv(f,N),m=this.recurrentConv(m,F),b=this.recurrentConv(b,D),y=this.recurrentConv(y,R);const E=this.recurrentActivation.apply(s.WQq(h,f)),x=this.recurrentActivation.apply(s.WQq(u,m)),Q=s.WQq(s.lKK(x,a),s.lKK(E,this.activation.apply(s.WQq(p,b)))),L=s.lKK(this.recurrentActivation.apply(s.WQq(d,y)),this.activation.apply(Q));return[L,L,Q]}))}getConfig(){const t=super.getConfig(),{units:e}=t,i=function(t,e){var i={};for(var s in t)Object.prototype.hasOwnProperty.call(t,s)&&e.indexOf(s)<0&&(i[s]=t[s]);if(null!=t&&"function"==typeof Object.getOwnPropertySymbols){var n=0;for(s=Object.getOwnPropertySymbols(t);n<s.length;n++)e.indexOf(s[n])<0&&Object.prototype.propertyIsEnumerable.call(t,s[n])&&(i[s[n]]=t[s[n]])}return i}(t,["units"]),s={filters:this.filters,kernelSize:this.kernelSize,padding:this.padding,dataFormat:this.dataFormat,dilationRate:this.dilationRate,strides:this.strides};return Object.assign(Object.assign({},i),s)}inputConv(t,e,i,n){const r=s.Xtf(t,e,this.strides,n||"valid","channelsFirst"===this.dataFormat?"NCHW":"NHWC",this.dilationRate);return i?w.ni(r,i,this.dataFormat):r}recurrentConv(t,e){return s.Xtf(t,e,1,"same","channelsFirst"===this.dataFormat?"NCHW":"NHWC")}}dt.className="ConvLSTM2DCell",s.JFn.registerClass(dt);class gt extends pt{constructor(t){const e=new dt(t);super(Object.assign(Object.assign({},t),{cell:e}))}static fromConfig(t,e){return new t(e)}}gt.className="ConvLSTM2D",s.JFn.registerClass(gt);class ft extends o.Wd{constructor(t){super(t),this.rate=Math.max(Math.min(t.rate,1),0),this.noiseShape=t.noiseShape,this.seed=t.seed,this.supportsMasking=!0}getNoiseShape(t){if(null==this.noiseShape)return this.noiseShape;const e=t.shape,i=[];for(let t=0;t<this.noiseShape.length;++t)i.push(null==this.noiseShape[t]?e[t]:this.noiseShape[t]);return i}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t);if(0<this.rate&&this.rate<1){const t=null!=e.training&&e.training,s=this.getNoiseShape(i);return w.Ls((()=>w.EZ(i,this.rate,s,this.seed)),(()=>i),t)}return t}))}getConfig(){const t={rate:this.rate,noiseShape:this.noiseShape,seed:this.seed},e=super.getConfig();return Object.assign(t,e),t}dispose(){return super.dispose()}}ft.className="Dropout",s.JFn.registerClass(ft);class mt extends ft{constructor(t){super(t),this.inputSpec=[{ndim:3}]}getNoiseShape(t){const e=t.shape;return[e[0],1,e[2]]}}mt.className="SpatialDropout1D",s.JFn.registerClass(mt);class bt extends o.Wd{constructor(t){if(super(t),this.activation=null,this.useBias=!0,this.kernel=null,this.bias=null,this.DEFAULT_KERNEL_INITIALIZER="glorotNormal",this.DEFAULT_BIAS_INITIALIZER="zeros",null==t.batchInputShape&&null==t.inputShape&&null!=t.inputDim){let e=null;null!=t.batchSize&&(e=t.batchSize),this.batchInputShape=[e,t.inputDim]}this.units=t.units,(0,d.oo)(this.units,"units"),this.activation=(0,y.b_)(t.activation),null!=t.useBias&&(this.useBias=t.useBias),this.kernelInitializer=(0,a.Fe)(t.kernelInitializer||this.DEFAULT_KERNEL_INITIALIZER),this.biasInitializer=(0,a.Fe)(t.biasInitializer||this.DEFAULT_BIAS_INITIALIZER),this.kernelConstraint=(0,r.YZ)(t.kernelConstraint),this.biasConstraint=(0,r.YZ)(t.biasConstraint),this.kernelRegularizer=v(t.kernelRegularizer),this.biasRegularizer=v(t.biasRegularizer),this.activityRegularizer=v(t.activityRegularizer),this.supportsMasking=!0,this.inputSpec=[{minNDim:2}]}build(t){const e=(t=(0,f.U$)(t))[t.length-1];null==this.kernel&&(this.kernel=this.addWeight("kernel",[e,this.units],null,this.kernelInitializer,this.kernelRegularizer,!0,this.kernelConstraint),this.useBias&&(this.bias=this.addWeight("bias",[this.units],null,this.biasInitializer,this.biasRegularizer,!0,this.biasConstraint))),this.inputSpec=[{minNDim:2,axes:{[-1]:e}}],this.built=!0}computeOutputShape(t){const e=(t=(0,f.U$)(t)).slice();return e[e.length-1]=this.units,e}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t),s=(0,d.Cd)(this.activation.getClassName());let n;return null!=s?n=w.Om(i,this.kernel.read(),s,this.bias?this.bias.read():null):(n=w.Om(i,this.kernel.read()),null!=this.bias&&(n=w.ni(n,this.bias.read())),null!=this.activation&&(n=this.activation.apply(n))),n}))}getConfig(){const t={units:this.units,activation:(0,y.Bu)(this.activation),useBias:this.useBias,kernelInitializer:(0,a.zo)(this.kernelInitializer),biasInitializer:(0,a.zo)(this.biasInitializer),kernelRegularizer:C(this.kernelRegularizer),biasRegularizer:C(this.biasRegularizer),activityRegularizer:C(this.activityRegularizer),kernelConstraint:(0,r.uH)(this.kernelConstraint),biasConstraint:(0,r.uH)(this.biasConstraint)},e=super.getConfig();return Object.assign(t,e),t}}bt.className="Dense",s.JFn.registerClass(bt);class yt extends o.Wd{constructor(t){super(t=t||{}),this.inputSpec=[{minNDim:3}],this.dataFormat=t.dataFormat}computeOutputShape(t){t=(0,f.U$)(t);for(const e of t.slice(1))if(null==e)throw new c.Qp(`The shape of the input to "Flatten" is not fully defined (got ${t.slice(1)}). Make sure to pass a complete "input_shape" or "batch_input_shape" argument to the first layer in your model.`);return[t[0],(0,L.no)(t,1)]}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);let i=(0,f.un)(t);if("channelsFirst"===this.dataFormat&&i.rank>1){const t=[0];for(let e=2;e<i.rank;++e)t.push(e);t.push(1),i=(0,s.mgz)(i,t)}return w.PS(i)}))}getConfig(){const t={};null!=this.dataFormat&&(t.dataFormat=this.dataFormat);const e=super.getConfig();return Object.assign(t,e),t}}yt.className="Flatten",s.JFn.registerClass(yt);class wt extends o.Wd{constructor(t){super(t),this.supportsMasking=!0,this.activation=(0,y.b_)(t.activation)}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t);return this.activation.apply(i)}))}getConfig(){const t={activation:(0,y.Bu)(this.activation)},e=super.getConfig();return Object.assign(t,e),t}}wt.className="Activation",s.JFn.registerClass(wt);class zt extends o.Wd{constructor(t){super(t),this.n=t.n,this.inputSpec=[{ndim:2}]}computeOutputShape(t){return[t[0],this.n,t[1]]}call(t,e){return(0,s.DZQ)((()=>(t=(0,f.un)(t),w.ux(t,this.n))))}getConfig(){const t={n:this.n},e=super.getConfig();return Object.assign(t,e),t}}zt.className="RepeatVector",s.JFn.registerClass(zt);class St extends o.Wd{constructor(t){super(t),this.targetShape=t.targetShape;for(let t=0;t<this.targetShape.length;++t)this.isUnknown(this.targetShape[t])&&(this.targetShape[t]=null)}isUnknown(t){return t<0||null==t}fixUnknownDimension(t,e){const i="Total size of new array must be unchanged.",s=e.slice();let n=1,r=null;for(let t=0;t<s.length;++t){const e=s[t];if(this.isUnknown(e)){if(null!==r)throw new c.Qp("Can only specifiy one unknown dimension.");r=t}else n*=e}const a=(0,L.no)(t);if(null!==r){if(0===n||a%n!=0)throw new c.Qp(i);s[r]=a/n}else if(a!==n)throw new c.Qp(i);return s}computeOutputShape(t){let e=!1;for(let i=0;i<t.length;++i)if(this.isUnknown(t[i])){e=!0;break}return e?t.slice(0,1).concat(this.targetShape):t.slice(0,1).concat(this.fixUnknownDimension(t.slice(1),this.targetShape))}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t),n=i.shape,r=n.slice(0,1).concat(this.fixUnknownDimension(n.slice(1),this.targetShape));return(0,s.tQQ)(i,r)}))}getConfig(){const t={targetShape:this.targetShape},e=super.getConfig();return Object.assign(t,e),t}}St.className="Reshape",s.JFn.registerClass(St);class kt extends o.Wd{constructor(t){if(super(t),null==t.dims)throw new Error("Required configuration field `dims` is missing during Permute constructor call.");if(!Array.isArray(t.dims))throw new Error(`Permute constructor requires \`dims\` to be an Array, but received ${t.dims} instead.`);const e=(0,L.y1)(1,t.dims.length+1);if(!s.ZSL.arraysEqual(t.dims.slice().sort(),e))throw new Error("Invalid permutation `dims`: "+JSON.stringify(t.dims)+" `dims` must contain consecutive integers starting from 1.");this.dims=t.dims,this.dimsIncludingBatch=[0].concat(this.dims),this.inputSpec=[new o.eO({ndim:this.dims.length+1})]}computeOutputShape(t){const e=(t=(0,f.U$)(t)).slice();return this.dims.forEach(((i,s)=>{e[s+1]=t[i]})),e}call(t,e){return(0,s.mgz)((0,f.un)(t),this.dimsIncludingBatch)}getConfig(){const t={dims:this.dims},e=super.getConfig();return Object.assign(t,e),t}}kt.className="Permute",s.JFn.registerClass(kt);class Ct extends o.Wd{constructor(t){super(null==t?{}:t),this.supportsMasking=!0,this.maskValue=null!=t?null==t.maskValue?0:t.maskValue:0}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={maskValue:this.maskValue};return Object.assign(e,t),e}computeMask(t,e){const i=(0,f.un)(t);return(0,s.bzn)((0,s.Ec)(i,this.maskValue),-1)}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t),n=(0,s.bzn)((0,s.Ec)(i,this.maskValue),-1,!0);return(0,s.lKK)(i,(0,s.wgE)(n,i.dtype))}))}}Ct.className="Masking",s.JFn.registerClass(Ct);class At extends o.Wd{constructor(t){if(super(t),this.embeddings=null,this.DEFAULT_EMBEDDINGS_INITIALIZER="randomUniform",null==t.batchInputShape&&null==t.inputShape){let e=null;null!=t.batchSize&&(e=t.batchSize),null==t.inputLength?this.batchInputShape=[e,null]:this.batchInputShape=[e].concat(d.st(t.inputLength))}this.inputDim=t.inputDim,d.oo(this.inputDim,"inputDim"),this.outputDim=t.outputDim,d.oo(this.outputDim,"outputDim"),this.embeddingsInitializer=(0,a.Fe)(t.embeddingsInitializer||this.DEFAULT_EMBEDDINGS_INITIALIZER),this.embeddingsRegularizer=v(t.embeddingsRegularizer),this.activityRegularizer=v(t.activityRegularizer),this.embeddingsConstraint=(0,r.YZ)(t.embeddingsConstraint),this.maskZero=t.maskZero,this.supportsMasking=t.maskZero,this.inputLength=t.inputLength}build(t){this.embeddings=this.addWeight("embeddings",[this.inputDim,this.outputDim],this.dtype,this.embeddingsInitializer,this.embeddingsRegularizer,!0,this.embeddingsConstraint),this.built=!0}warnOnIncompatibleInputShape(t){}computeMask(t,e){return(0,s.DZQ)((()=>this.maskZero?(t=(0,f.un)(t),(0,s.Ec)(t,(0,s.POl)(t))):null))}computeOutputShape(t){if(t=(0,f.U$)(t),null==this.inputLength)return[...t,this.outputDim];const e=d.st(this.inputLength);if(e.length!==t.length-1)throw new c.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);{let i=0;for(let s=0;s<e.length;++s){const n=e[s],r=t[s+1];if(null!=n&&null!=r&&n!==r)throw new c.Qp(`"inputLength" is ${this.inputLength}, but received input shape has shape ${t}`);null==n&&(e[i]=r),i++}}return[t[0],...e,this.outputDim]}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);let i=(0,f.un)(t);"int32"!==i.dtype&&(i=w.wg(i,"int32"));const n=w.kg(this.embeddings.read(),(0,s.tQQ)(i,[i.size]));return(0,s.tQQ)(n,(0,f.U$)(this.computeOutputShape(i.shape)))}))}getConfig(){const t={inputDim:this.inputDim,outputDim:this.outputDim,embeddingsInitializer:(0,a.zo)(this.embeddingsInitializer),embeddingsRegularizer:C(this.embeddingsRegularizer),activityRegularizer:C(this.activityRegularizer),embeddingsConstraint:(0,r.uH)(this.embeddingsConstraint),maskZero:this.maskZero,inputLength:this.inputLength},e=super.getConfig();return Object.assign(t,e),t}}At.className="Embedding",s.JFn.registerClass(At);var vt=i(48981);class It extends o.Wd{constructor(t){super(t||{}),this.supportsMasking=!0}mergeFunction(t){throw new c.EH}computeElementwiseOpOutputShape(t,e){if(null==t||null==e)return null;if(t.length<e.length)return this.computeElementwiseOpOutputShape(e,t);if(0===e.length)return t;const i=t.slice(0,t.length-e.length);for(let s=0;s<e.length;++s){const n=t[t.length-e.length+s],r=e[s];if(null==n||null==r||n<0||r<0)i.push(null);else if(1===n)i.push(r);else if(1===r)i.push(n);else{if(n!==r)throw new c.Qp("Operands could not be broadcast together with shapes "+JSON.stringify(t)+" "+JSON.stringify(e));i.push(n)}}return i}build(t){if(Array.isArray(t)&&!Array.isArray(t[0])&&(t=[(0,f.U$)(t)]),t.length<2)throw new c.Qp(`A merge layer should be called on an Array of at least 2 inputs. Got ${t.length} input(s).`);let e=[];for(const i of t)null!=i&&null!==i[0]&&e.push(i[0]);if(e=d.Am(e),e.length>1)throw new c.Qp(`Can not merge tensors with different batch sizes. Got tensors with shapes: ${JSON.stringify(t)}.`);let i=null==t[0]?null:t[0].slice(1);for(let e=1;e<t.length;++e){const s=null==t[e]?null:t[e].slice(1);i=this.computeElementwiseOpOutputShape(i,s)}const s=t.map((t=>t.length));-1===t.indexOf(null)&&1===d.Am(s).length?this.reshapeRequired=!1:this.reshapeRequired=!0}call(t,e){return(0,s.DZQ)((()=>{if(this.reshapeRequired){const e=[],i=t.map((t=>t.rank));if(-1===i.indexOf(null)){const s=L.T9(i);for(let i of t){const t=i.rank;for(let e=0;e<s-t;++e)i=w.UG(i,1);e.push(i)}return this.mergeFunction(e)}{let i=!1;for(const n of t){const t=n.rank;if(null==t){const t=n.shape,r=t[0],a=t.slice(1).concat([r]);let l=s.tQQ(n,[r].concat(L.no(t.slice(1))));l=s.mgz(l,[1,0]),l=s.tQQ(l,a),e.push(l),i=!0}else if(t>1){const r=L.y1(1,t).concat([0]);e.push(s.mgz(n,r)),i=!0}else e.push(n)}let n=this.mergeFunction(e);const r=n.rank;if(i)if(null==r){const t=n.shape,e=t[t.length-1],i=[e].concat(t.slice(0,t.length-1));n=s.tQQ(s.mgz(s.tQQ(n,[-1,e]),[1,0]),i)}else if(r>1){const t=[r-1].concat(L.y1(0,r-1));n=s.mgz(n,t)}return n}}return this.mergeFunction(t)}))}computeOutputShape(t){let e;e=null==t[0]?null:t[0].slice(1);for(let i=1;i<t.length;++i){const s=null==t[i]?null:t[i].slice(1);e=this.computeElementwiseOpOutputShape(e,s)}let i=[];for(const e of t)null!=e&&null!==e[0]&&i.push(e[0]);return i=d.Am(i),e=1===i.length?i.concat(e):[null].concat(e),e}computeMask(t,e){return s.DZQ((()=>{if(null==e)return null;if(!Array.isArray(e))throw new c.Qp("`mask` should be an Array");if(!Array.isArray(t))throw new c.Qp("`inputs` should be an Array");if(e.length!==t.length)throw new c.Qp(`The Array 'inputs' and 'mask' are expected to have the same length, but have different lengths (${t.length} vs ${e.length})`);if(e.every((t=>null==t)))return null;let i=(e=e.map((t=>null==t?t:s.UG6(t,0))))[0];for(let t=1;t<e.length-1;++t)i=s.n76(i,e[t]);return i}))}}class Nt extends It{constructor(t){super(t)}mergeFunction(t){return(0,s.DZQ)((()=>{let e=t[0].clone();for(let i=1;i<t.length;++i)e=s.WQq(e,t[i]);return e}))}}Nt.className="Add",s.JFn.registerClass(Nt);class Ft extends It{constructor(t){super(t)}mergeFunction(t){return(0,s.DZQ)((()=>{let e=t[0].clone();for(let i=1;i<t.length;++i)e=s.lKK(e,t[i]);return e}))}}Ft.className="Multiply",s.JFn.registerClass(Ft);class Dt extends It{constructor(t){super(t)}mergeFunction(t){return(0,s.DZQ)((()=>{let e=t[0].clone();for(let i=1;i<t.length;++i)e=s.WQq(e,t[i]);return s.lKK(1/t.length,e)}))}}Dt.className="Average",s.JFn.registerClass(Dt);class Rt extends It{constructor(t){super(t)}mergeFunction(t){return(0,s.DZQ)((()=>{let e=t[0];for(let i=1;i<t.length;++i)e=s.PhQ(e,t[i]);return e}))}}Rt.className="Maximum",s.JFn.registerClass(Rt);class Et extends It{constructor(t){super(t)}mergeFunction(t){return(0,s.DZQ)((()=>{let e=t[0];for(let i=1;i<t.length;++i)e=s.BpO(e,t[i]);return e}))}}Et.className="Minimum",s.JFn.registerClass(Et);class xt extends It{constructor(t){super(t),this.DEFAULT_AXIS=-1,null==t&&(t={}),this.axis=null==t.axis?this.DEFAULT_AXIS:t.axis,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){if(!Array.isArray(t)||!Array.isArray(t[0])||1===t.length)throw new c.Qp("A `Concatenate` layer should be called on a list of at least 2 inputs");let e=!0;for(const i of t)if(null!=i){e=!1;break}if(e)return;const i=[];for(let e=0;e<t.length;++e){const n=t[e].slice();n.splice(this.axis,1);let r=!1;for(const t of i)if(s.ZSL.arraysEqual(t,n)){r=!0;break}r||i.push(n)}if(i.length>1)throw new c.Qp("A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got input shapes: "+JSON.stringify(t))}mergeFunction(t){return(0,s.DZQ)((()=>w.u1(t,this.axis)))}computeOutputShape(t){if(!Array.isArray(t)||!Array.isArray(t[0]))throw new c.Qp("A `Concatenate` layer should be called on a list of inputs.");const e=t,i=e[0].slice(),s=this.axis<0?i.length+this.axis:this.axis;for(const t of e.slice(1)){if(null==i[s]||null==t[s]){i[s]=null;break}i[s]+=t[s]}return i}computeMask(t,e){if(null==e)return null;if(!Array.isArray(e))throw new c.Qp("`mask` should be an array for Concatenate");if(!Array.isArray(t))throw new c.Qp("`inputs` should be an array for Concatenate");if(e.length!==t.length)throw new c.Qp(`Mismatch in the length of mask (${e.length}) and the legnth of inputs (${t.length})`);return s.DZQ((()=>{let i=!0;if(e.forEach((t=>{null==t||(i=!1)})),i)return null;const n=[];for(let i=0;i<t.length;++i)null==e[i]?n.push(s.wgE(s.P61(t[i]),"bool")):e[i].rank<t[i].rank?n.push(s.UG6(e[i],-1)):n.push(e[i]);const r=s.xWs(n,this.axis);return s.Q7R(r,-1,!1)}))}getConfig(){const t={axis:this.axis},e=super.getConfig();return Object.assign(t,e),t}}function Qt(t,e){for(;t<0;)t+=e;return t}xt.className="Concatenate",s.JFn.registerClass(xt);class Lt extends It{constructor(t){super(t),this.axes=t.axes,this.normalize=null!=t.normalize&&t.normalize,this.supportsMasking=!0,this.reshapeRequired=!1}build(t){s.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0],i=t[1];if(e.length>3||i.length>3)throw new c.EH("Dot layer does not support tensors of 4D or higher rank yet.");const n=this.interpretAxes(e,i);if(e[n[0]]!==i[n[1]])throw new c.Qp(`Dimension incompatibility: ${e[n[0]]} !== ${i[n[1]]}`)}mergeFunction(t){if(2!==t.length)throw new c.Qp(`A \`Dot\` layer must be called on exactly 2 inputs, but received ${t.length} input(s).`);let e,i=t[0],n=t[1];return e=Array.isArray(this.axes)?this.axes.map(((e,i)=>Qt(e,t[i].shape.length))):[Qt(this.axes,i.shape.length),Qt(this.axes,n.shape.length)],this.normalize&&(i=(0,vt.Yq)(i,e[0]),n=(0,vt.Yq)(n,e[1])),function(t,e,i){if(t.shape.length>3||e.shape.length>3)throw new c.EH("batchDot is not implemented for tensors of 4D or higher rank yet");if(s.ZSL.assert(t.shape.length>=2,(()=>`batchDot requires the rank of x to be >= 2, but got ${t.shape.length}`)),s.ZSL.assert(t.shape.length>=2,(()=>`batchDot requires the rank of y to be >= 2, but got ${e.shape.length}`)),"number"==typeof i&&(i=[i,i]),"complex64"===t.dtype||"complex64"===e.dtype)throw new c.EH("batchDot is not implemented for complex64-type Tensors yet.");const n=t.shape.length,r=e.shape.length;null==i&&(i=[n-1,r-2]);const a=i;return s.DZQ((()=>{let i,l;if(n>r){i=n-r;const t=[];for(let e=0;e<i;++e)t.push(1);e=s.tQQ(e,e.shape.concat(t))}else if(r>n){i=r-n;const e=[];for(let t=0;t<i;++t)e.push(1);t=s.tQQ(t,t.shape.concat(e))}else i=0;if(2===t.shape.length&&2===e.shape.length)l=a[0]===a[1]?s.czq(s.lKK(t,e),a[0]):s.czq(s.lKK(s.mgz(t,[1,0]),e),a[1]);else{const i=a[0]!==t.shape.length-1,n=a[1]===e.shape.length-1;l=s.NoW(t,e,i,n)}if(i>0){let t;t=n>r?n+r-3:n-1;const e=[];for(let s=t;s<t+i;++s)e.push(s);l=s.r2V(l,e)}return 1===l.shape.length&&(l=s.UG6(l,1)),l}))}(i,n,e)}interpretAxes(t,e){let i;return i=Array.isArray(this.axes)?this.axes:[Qt(this.axes,t.length),Qt(this.axes,e.length)],i}computeOutputShape(t){s.ZSL.assert(Array.isArray(t)&&2===t.length&&Array.isArray(t[0])&&Array.isArray(t[1]),(()=>"A `Dot` layer should be called on a list of exactly 2 inputs."));const e=t[0].slice(),i=t[1].slice();if(e.length>3||i.length>3)throw new c.EH("Dot layer does not support tensors of 4D or higher rank yet.");const n=this.interpretAxes(e,i);e.splice(n[0],1),i.splice(n[1],1),i.splice(0,1);const r=e.concat(i);return 1===r.length&&r.push(1),r}computeMask(t,e){return null}getConfig(){const t={axes:this.axes,normalize:this.normalize},e=super.getConfig();return Object.assign(t,e),t}}Lt.className="Dot",s.JFn.registerClass(Lt);class Tt extends o.Wd{constructor(t){super(t),this.supportsMasking=!0,this.stddev=t.stddev}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={stddev:this.stddev};return Object.assign(e,t),e}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t);return w.Ls((()=>(0,s.WQq)(w.FE(i.shape,0,this.stddev),i)),(()=>i),e.training||!1)}))}}Tt.className="GaussianNoise",s.JFn.registerClass(Tt);class Ot extends o.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e);const i=(0,f.un)(t);if(this.rate>0&&this.rate<1){const t=()=>{const t=Math.sqrt(this.rate/(1-this.rate));return(0,s.lKK)(i,w.FE(i.shape,1,t))};return w.Ls(t,(()=>i),e.training||!1)}return i}))}}Ot.className="GaussianDropout",s.JFn.registerClass(Ot);class Mt extends o.Wd{constructor(t){super(t),this.supportsMasking=!0,this.rate=t.rate,this.noiseShape=t.noiseShape}_getNoiseShape(t){return this.noiseShape||(0,f.un)(t).shape}computeOutputShape(t){return t}getConfig(){const t=super.getConfig(),e={rate:this.rate};return Object.assign(e,t),e}call(t,e){return(0,s.DZQ)((()=>{if(this.rate<1&&this.rate>0){const i=this._getNoiseShape(t),n=()=>{const e=(0,f.un)(t),n=-1.7580993408473766;let r=(0,s.DQN)((0,s.YeY)(i),this.rate);r=w.wg(r,"float32");const a=((1-this.rate)*(1+this.rate*n**2))**-.5,l=-a*n*this.rate,o=(0,s.WQq)((0,s.lKK)(e,r),(0,s.lKK)((0,s.WQq)(r,-1),n));return(0,s.WQq)((0,s.lKK)(o,a),l)};return w.Ls(n,(()=>(0,f.un)(t)),e.training||!1)}return t}))}}function $t(t,e,i,n,r,a=.001){let l;if(2===t.rank)l=s.BFc(t,e,i,n,r,a);else if(3===t.rank)l=s.kSi(t,e,i,n,r,a);else{if(4!==t.rank)throw new c.EH(`batchNormalization is not implemented for array of rank ${t.rank} yet`);l=s.T5N(t,e,i,n,r,a)}return l}Mt.className="AlphaDropout",s.JFn.registerClass(Mt);class _t extends o.Wd{constructor(t){null==t&&(t={}),super(t),this.supportsMasking=!0,this.axis=null==t.axis?-1:t.axis,this.momentum=null==t.momentum?.99:t.momentum,this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,a.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,a.Fe)(t.gammaInitializer||"ones"),this.movingMeanInitializer=(0,a.Fe)(t.movingMeanInitializer||"zeros"),this.movingVarianceInitializer=(0,a.Fe)(t.movingVarianceInitializer||"ones"),this.betaConstraint=(0,r.YZ)(t.betaConstraint),this.gammaConstraint=(0,r.YZ)(t.gammaConstraint),this.betaRegularizer=v(t.betaRegularizer),this.gammaRegularizer=v(t.gammaRegularizer)}build(t){t=(0,f.U$)(t);const e=this.axis>=0?this.axis:this.axis+t.length,i=t[e];if(null==i)throw new c.Qp(`Axis ${e} of input tensor should have a defined dimension but the layer received an input with shape ${JSON.stringify(t)}.`);this.inputSpec=[new o.eO({ndim:t.length,axes:{[e]:i}})];const s=[i];this.scale&&(this.gamma=this.addWeight("gamma",s,null,this.gammaInitializer,this.gammaRegularizer,!0,this.gammaConstraint)),this.center&&(this.beta=this.addWeight("beta",s,null,this.betaInitializer,this.betaRegularizer,!0,this.betaConstraint)),this.movingMean=this.addWeight("moving_mean",s,null,this.movingMeanInitializer,null,!1),this.movingVariance=this.addWeight("moving_variance",s,null,this.movingVarianceInitializer,null,!1),this.built=!0}call(t,e){return(0,s.DZQ)((()=>{const i=null!=e.training&&e.training,n=(0,f.un)(t),r=n.shape,a=r.length,l=L.y1(0,a),o=this.axis>=0?this.axis:this.axis+a;l.splice(o,1);const h=d.fD(1,a);h[o]=r[o];const u=l.slice();u.sort();const c=!s.ZSL.arraysEqual(u,L.y1(0,a).slice(0,a-1));if(!i)return(()=>{if(c){const t=(0,s.tQQ)(this.movingMean.read(),h),e=(0,s.tQQ)(this.movingVariance.read(),h),i=this.center?(0,s.tQQ)(this.beta.read(),h):null,r=this.scale?(0,s.tQQ)(this.gamma.read(),h):null;return $t(n,t,e,i,r,this.epsilon)}return $t(n,this.movingMean.read(),this.movingVariance.read(),null==this.beta?null:this.beta.read(),null==this.gamma?null:this.gamma.read(),this.epsilon)})();const[p,g,m]=function(t,e,i,n,r=.001){return s.ZSL.arraysEqual(n.slice().sort(),L.y1(0,t.rank-1))?function(t,e,i,n,r=.001){return(0,s.DZQ)((()=>{const a=s.Clk(t,n),l=a.mean,o=a.variance;return[$t(t,l,o,i,e,r),l,o]}))}(t,e,i,n,r):function(t,e,i,n,r=.001){return(0,s.DZQ)((()=>{const a=s.Clk(t,n),l=a.mean,o=a.variance,h=[];for(const e of L.y1(0,t.rank))-1!==n.indexOf(e)?h.push(1):h.push(t.shape[e]);const u=(0,s.tQQ)(l,h),c=(0,s.tQQ)(o,h),p=null==e?null:(0,s.tQQ)(e,h),d=null==i?null:(0,s.tQQ)(i,h);return[$t(t,u,c,d,p,r),l,o]}))}(t,e,i,n,r)}(n,this.gamma.read(),this.beta.read(),l,this.epsilon),b=(t,e,i)=>{s.DZQ((()=>{const n=1-i,r=t.read(),a=s.lKK(s.jbE(r,e),n);t.write(s.jbE(r,a))}))};return(()=>{b(this.movingMean,g,this.momentum),b(this.movingVariance,m,this.momentum)})(),p}))}getConfig(){const t={axis:this.axis,momentum:this.momentum,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,a.zo)(this.betaInitializer),gammaInitializer:(0,a.zo)(this.gammaInitializer),movingMeanInitializer:(0,a.zo)(this.movingMeanInitializer),movingVarianceInitializer:(0,a.zo)(this.movingVarianceInitializer),betaRegularizer:C(this.betaRegularizer),gammaRegularizer:C(this.gammaRegularizer),betaConstraint:(0,r.uH)(this.betaConstraint),gammaConstraint:(0,r.uH)(this.gammaConstraint)},e=super.getConfig();return Object.assign(t,e),t}}_t.className="BatchNormalization",s.JFn.registerClass(_t);class Ut extends o.Wd{constructor(t){if(null==t&&(t={}),super(t),this.axis=null==t.axis?-1:t.axis,"number"==typeof this.axis){if(!Number.isInteger(this.axis))throw new Error(`Expected axis to be an integer, but received ${this.axis}`)}else{if(!Array.isArray(this.axis))throw new Error(`Expected axis to be an integer or an array of integers, but received ${JSON.stringify(this.axis)}`);for(const t of this.axis)if(!Number.isInteger(t))throw new Error(`Expected axis to be an array of integers, but received ${JSON.stringify(this.axis)}`)}this.epsilon=null==t.epsilon?.001:t.epsilon,this.center=null==t.center||t.center,this.scale=null==t.scale||t.scale,this.betaInitializer=(0,a.Fe)(t.betaInitializer||"zeros"),this.gammaInitializer=(0,a.Fe)(t.gammaInitializer||"ones"),this.betaRegularizer=v(t.betaRegularizer),this.gammaRegularizer=v(t.gammaRegularizer),this.supportsMasking=!0}build(t){const e=(t=(0,f.U$)(t)).length;"number"==typeof this.axis&&(this.axis=[this.axis]);for(let t=0;t<this.axis.length;++t)this.axis[t]<0&&(this.axis[t]+=e);for(const t of this.axis)if(t<0||t>=e)throw new Error(`Invalid axis: ${t}`);if(this.axis.length!==d.Am(this.axis).length)throw new Error(`Found duplicate axes in: ${this.axis}`);const i=this.axis.map((e=>t[e]));this.scale?this.gamma=this.addWeight("gamma",i,"float32",this.gammaInitializer,this.gammaRegularizer,!0):this.gamma=null,this.center?this.beta=this.addWeight("beta",i,"float32",this.betaInitializer,this.betaRegularizer,!0):this.beta=null,this.built=!0}call(t,e){const i=(0,f.un)(t),n=i.shape,r=n.length;return(0,s.DZQ)((()=>{let{mean:t,variance:e}=(0,s.Clk)(i,this.axis,!0);const a=d.fD(1,r);for(const t of this.axis)a[t]=n[t];const l=t=>null!=t&&t.shape.length!==r?s.tQQ(t,a):t;let o=this.scale?l(this.gamma.read()):null,h=this.center?l(this.beta.read()):null;const u=[],c=[];for(let t=0;t<r;++t)-1!==this.axis.indexOf(t)?(u.push(n[t]),c.push(1)):(u.push(1),c.push(n[t]));return t=s.Vsq(t,u),e=s.Vsq(e,u),null!=o&&(o=s.Vsq(o,c)),null!=h&&(h=s.Vsq(h,c)),$t(i,t,e,h,o,this.epsilon)}))}getConfig(){const t={axis:this.axis,epsilon:this.epsilon,center:this.center,scale:this.scale,betaInitializer:(0,a.zo)(this.betaInitializer),gammaInitializer:(0,a.zo)(this.gammaInitializer),betaRegularizer:C(this.betaRegularizer),gammaRegularizer:C(this.gammaRegularizer)},e=super.getConfig();return Object.assign(t,e),t}}Ut.className="LayerNormalization",s.JFn.registerClass(Ut);class Zt extends o.Wd{constructor(t){if(null==t&&(t={}),super(t),this.dataFormat=null==t.dataFormat?(0,x.VI)():t.dataFormat,null==t.padding)this.padding=[[1,1],[1,1]];else if("number"==typeof t.padding)this.padding=[[t.padding,t.padding],[t.padding,t.padding]];else{if(t.padding=t.padding,2!==t.padding.length)throw new c.Qp(`ZeroPadding2D expects padding to be a length-2 array, but received a length-${t.padding.length} array.`);let e,i;if("number"==typeof t.padding[0])e=[t.padding[0],t.padding[0]],i=[t.padding[1],t.padding[1]];else{if(t.padding=t.padding,2!==t.padding[0].length)throw new c.Qp(`ZeroPadding2D expects height padding to be a length-2 array, but received a length-${t.padding[0].length} array.`);if(e=t.padding[0],2!==t.padding[1].length)throw new c.Qp(`ZeroPadding2D expects width padding to be a length-2 array, but received a length-${t.padding[1].length} array.`);i=t.padding[1]}this.padding=[e,i]}this.inputSpec=[new o.eO({ndim:4})]}computeOutputShape(t){let e,i;return t=(0,f.U$)(t),"channelsFirst"===this.dataFormat?(e=null!=t[2]&&t[2]>=0?t[2]+this.padding[0][0]+this.padding[0][1]:null,i=null!=t[3]&&t[3]>=0?t[3]+this.padding[1][0]+this.padding[1][1]:null,[t[0],t[1],e,i]):(e=null!=t[1]&&t[1]>=0?t[1]+this.padding[0][0]+this.padding[0][1]:null,i=null!=t[2]&&t[2]>=0?t[2]+this.padding[1][0]+this.padding[1][1]:null,[t[0],e,i,t[3]])}call(t,e){return(0,s.DZQ)((()=>{return e=(0,f.un)(t),i=this.padding,n=this.dataFormat,(0,s.DZQ)((()=>{if(4!==e.rank)throw new c.Qp(`temporalPadding expects input tensor to be 4-D, but received a ${e.rank}-D tensor.`);if(null==i&&(i=[[1,1],[1,1]]),2!==i.length||2!==i[0].length||2!==i[1].length)throw new c.Qp("spatial2dPadding expects `padding` to be an Array of two Arrays, each of which is an Array of two integers.");if(null==n&&(n=(0,x.VI)()),"channelsLast"!==n&&"channelsFirst"!==n)throw new c.Qp(`Unknown data format: ${n}. Supported data formats are 'channelsLast' and 'channelsFirst.`);let t;return t="channelsFirst"===n?[[0,0],[0,0],i[0],i[1]]:[[0,0],i[0],i[1],[0,0]],s.eVF(e,t)}));var e,i,n}))}getConfig(){const t={padding:this.padding,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}function Wt(t,e,i,n,r,a){return(0,s.DZQ)((()=>{let l;(0,Q.uM)(r),(0,Q.Kx)(a),(0,Q.tB)(n),null==i&&(i=[1,1]),null==n&&(n="valid"),null==r&&(r=(0,x.VI)()),null==a&&(a="max"),t=$(t,r);const o="same"===n?"same":"valid";return l="max"===a?s.jgi(t,e,i,o):s.$jT(t,e,i,o),"channelsFirst"===r&&(l=s.mgz(l,[0,3,1,2])),l}))}function Bt(t,e,i,n,r,a){return(0,s.DZQ)((()=>{let l;(0,Q.uM)(r),(0,Q.Kx)(a),(0,Q.tB)(n),null==i&&(i=[1,1,1]),null==n&&(n="valid"),null==r&&(r=(0,x.VI)()),null==a&&(a="max"),t=_(t,r);const o="same"===n?"same":"valid";return l="max"===a?s.NYV(t,e,i,o):s.sub(t,e,i,o),"channelsFirst"===r&&(l=s.mgz(l,[0,4,1,2,3])),l}))}Zt.className="ZeroPadding2D",s.JFn.registerClass(Zt);class Kt extends o.Wd{constructor(t){if(null==t.poolSize&&(t.poolSize=2),super(t),"number"==typeof t.poolSize)this.poolSize=[t.poolSize];else{if(!Array.isArray(t.poolSize)||1!==t.poolSize.length||"number"!=typeof t.poolSize[0])throw new c.Qp(`poolSize for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.poolSize)}`);this.poolSize=t.poolSize}if((0,d.oo)(this.poolSize,"poolSize"),null==t.strides)this.strides=this.poolSize;else if("number"==typeof t.strides)this.strides=[t.strides];else{if(!Array.isArray(t.strides)||1!==t.strides.length||"number"!=typeof t.strides[0])throw new c.Qp(`strides for 1D convolutional layer must be a number or an Array of a single number, but received ${JSON.stringify(t.strides)}`);this.strides=t.strides}(0,d.oo)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,(0,Q.tB)(this.padding),this.inputSpec=[new o.eO({ndim:3})]}computeOutputShape(t){const e=O((t=(0,f.U$)(t))[1],this.poolSize[0],this.padding,this.strides[0]);return[t[0],e,t[2]]}call(t,e){return(0,s.DZQ)((()=>{this.invokeCallHook(t,e),t=w.UG((0,f.un)(t),2);const i=this.poolingFunction((0,f.un)(t),[this.poolSize[0],1],[this.strides[0],1],this.padding,"channelsLast");return s.r2V(i,[2])}))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides},e=super.getConfig();return Object.assign(t,e),t}}class jt extends Kt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Wt(t,e,i,s,n,"max")}}jt.className="MaxPooling1D",s.JFn.registerClass(jt);class Jt extends Kt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Wt(t,e,i,s,n,"avg")}}Jt.className="AveragePooling1D",s.JFn.registerClass(Jt);class qt extends o.Wd{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(2!==t.strides.length)throw new c.Qp(`If the strides property of a 2D pooling layer is an Array, it is expected to have a length of 2, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides];(0,d.oo)(this.poolSize,"poolSize"),(0,d.oo)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,Q.uM)(this.dataFormat),(0,Q.tB)(this.padding),this.inputSpec=[new o.eO({ndim:4})]}computeOutputShape(t){t=(0,f.U$)(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],i="channelsFirst"===this.dataFormat?t[3]:t[2];return e=O(e,this.poolSize[0],this.padding,this.strides[0]),i=O(i,this.poolSize[1],this.padding,this.strides[1]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,i]:[t[0],e,i,t[3]]}call(t,e){return(0,s.DZQ)((()=>(this.invokeCallHook(t,e),this.poolingFunction((0,f.un)(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Ht extends qt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Wt(t,e,i,s,n,"max")}}Ht.className="MaxPooling2D",s.JFn.registerClass(Ht);class Pt extends qt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Wt(t,e,i,s,n,"avg")}}Pt.className="AveragePooling2D",s.JFn.registerClass(Pt);class Vt extends o.Wd{constructor(t){if(null==t.poolSize&&(t.poolSize=[2,2,2]),super(t),this.poolSize=Array.isArray(t.poolSize)?t.poolSize:[t.poolSize,t.poolSize,t.poolSize],null==t.strides)this.strides=this.poolSize;else if(Array.isArray(t.strides)){if(3!==t.strides.length)throw new c.Qp(`If the strides property of a 3D pooling layer is an Array, it is expected to have a length of 3, but received length ${t.strides.length}.`);this.strides=t.strides}else this.strides=[t.strides,t.strides,t.strides];(0,d.oo)(this.poolSize,"poolSize"),(0,d.oo)(this.strides,"strides"),this.padding=null==t.padding?"valid":t.padding,this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,Q.uM)(this.dataFormat),(0,Q.tB)(this.padding),this.inputSpec=[new o.eO({ndim:5})]}computeOutputShape(t){t=(0,f.U$)(t);let e="channelsFirst"===this.dataFormat?t[2]:t[1],i="channelsFirst"===this.dataFormat?t[3]:t[2],s="channelsFirst"===this.dataFormat?t[4]:t[3];return e=O(e,this.poolSize[0],this.padding,this.strides[0]),i=O(i,this.poolSize[1],this.padding,this.strides[1]),s=O(s,this.poolSize[2],this.padding,this.strides[2]),"channelsFirst"===this.dataFormat?[t[0],t[1],e,i,s]:[t[0],e,i,s,t[4]]}call(t,e){return(0,s.DZQ)((()=>(this.invokeCallHook(t,e),this.poolingFunction((0,f.un)(t),this.poolSize,this.strides,this.padding,this.dataFormat))))}getConfig(){const t={poolSize:this.poolSize,padding:this.padding,strides:this.strides,dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class Gt extends Vt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Bt(t,e,i,s,n,"max")}}Gt.className="MaxPooling3D",s.JFn.registerClass(Gt);class Yt extends Vt{constructor(t){super(t)}poolingFunction(t,e,i,s,n){return(0,Q.uM)(n),(0,Q.tB)(s),Bt(t,e,i,s,n,"avg")}}Yt.className="AveragePooling3D",s.JFn.registerClass(Yt);class Xt extends o.Wd{constructor(t){super(t),this.inputSpec=[new o.eO({ndim:3})]}computeOutputShape(t){return[t[0],t[2]]}call(t,e){throw new c.EH}}class te extends Xt{constructor(t){super(t||{})}call(t,e){return(0,s.DZQ)((()=>{const e=(0,f.un)(t);return s.i2o(e,1)}))}}te.className="GlobalAveragePooling1D",s.JFn.registerClass(te);class ee extends Xt{constructor(t){super(t||{})}call(t,e){return(0,s.DZQ)((()=>{const e=(0,f.un)(t);return s.T9B(e,1)}))}}ee.className="GlobalMaxPooling1D",s.JFn.registerClass(ee);class ie extends o.Wd{constructor(t){super(t),this.dataFormat=null==t.dataFormat?"channelsLast":t.dataFormat,(0,Q.uM)(this.dataFormat),this.inputSpec=[new o.eO({ndim:4})]}computeOutputShape(t){return"channelsLast"===this.dataFormat?[t[0],t[3]]:[t[0],t[1]]}call(t,e){throw new c.EH}getConfig(){const t={dataFormat:this.dataFormat},e=super.getConfig();return Object.assign(t,e),t}}class se extends ie{call(t,e){return(0,s.DZQ)((()=>{const e=(0,f.un)(t);return"channelsLast"===this.dataFormat?s.i2o(e,[1,2]):s.i2o(e,[2,3])}))}}se.className="GlobalAveragePooling2D",s.JFn.registerClass(se);class ne extends ie{call(t,e){return(0,s.DZQ)((()=>{const e=(0,f.un)(t);return"channelsLast"===this.dataFormat?s.T9B(e,[1,2]):s.T9B(e,[2,3])}))}}ne.className="GlobalMaxPooling2D",s.JFn.registerClass(ne);var re=i(84604);class ae extends o.Wd{constructor(t){super(t),this.layer=t.layer}build(t){this.built=!0}get trainable(){return null!=this.layer&&this.layer.trainable}set trainable(t){null!=this.layer&&(this.layer.trainable=t)}get trainableWeights(){return this.layer.trainableWeights}get nonTrainableWeights(){return this.layer.nonTrainableWeights}get updates(){return this.layer._updates}get losses(){return this.layer.losses}getWeights(){return this.layer.getWeights()}setWeights(t){this.layer.setWeights(t)}getConfig(){const t={layer:{className:this.layer.getClassName(),config:this.layer.getConfig()}},e=super.getConfig();return Object.assign(t,e),t}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.layer&&this.layer.setFastWeightInitDuringBuild(t)}static fromConfig(t,e,i={}){const s=e.layer,n=(0,p.i)(s,i);delete e.layer;const r={layer:n};return Object.assign(r,e),new t(r)}}class le extends ae{constructor(t){super(t),this.supportsMasking=!0}build(t){if((t=(0,f.U$)(t)).length<3)throw new c.Qp(`TimeDistributed layer expects an input shape >= 3D, but received input shape ${JSON.stringify(t)}`);this.inputSpec=[{shape:t}];const e=[t[0]].concat(t.slice(2));this.layer.built||(this.layer.build(e),this.layer.built=!0),super.build(t)}computeOutputShape(t){const e=[(t=(0,f.U$)(t))[0]].concat(t.slice(2)),i=this.layer.computeOutputShape(e),s=t[1];return[i[0],s].concat(i.slice(1))}call(t,e){return(0,s.DZQ)((()=>et(((t,i)=>[(0,f.un)(this.layer.call(t,e)),[]]),t=(0,f.un)(t),[],!1,null,null,!1,!0)[1]))}}le.className="TimeDistributed",s.JFn.registerClass(le);class oe extends ae{constructor(t){super(t);const e=t.layer.getConfig(),i={};i.className=t.layer.getClassName(),i.config=e,this.forwardLayer=(0,p.i)(i),e.goBackwards=!0!==e.goBackwards;const s={};var n;if(s.className=t.layer.getClassName(),s.config=e,this.backwardLayer=(0,p.i)(s),this.forwardLayer.name="forward_"+this.forwardLayer.name,this.backwardLayer.name="backward_"+this.backwardLayer.name,this.mergeMode=void 0===t.mergeMode?"concat":t.mergeMode,n=this.mergeMode,d.E6(re.r$,"BidirectionalMergeMode",n),t.weights)throw new c.EH("weights support is not implemented for Bidirectional layer yet.");this._stateful=t.layer.stateful,this.returnSequences=t.layer.returnSequences,this.returnState=t.layer.returnState,this.supportsMasking=!0,this._trainable=!0,this.inputSpec=t.layer.inputSpec,this.numConstants=null}get trainable(){return this._trainable}set trainable(t){this._trainable=t,null!=this.forwardLayer&&(this.forwardLayer.trainable=t),null!=this.backwardLayer&&(this.backwardLayer.trainable=t)}getWeights(){return this.forwardLayer.getWeights().concat(this.backwardLayer.getWeights())}setWeights(t){const e=t.length,i=Math.floor(e/2);this.forwardLayer.setWeights(t.slice(0,i)),this.backwardLayer.setWeights(t.slice(i))}computeOutputShape(t){let e,i,s,n=this.forwardLayer.computeOutputShape(t);return Array.isArray(n)&&Array.isArray(n[0])||(n=[n]),this.returnState?(s=n.slice(1),e=n[0]):e=n[0],"concat"===this.mergeMode?(e[e.length-1]*=2,i=[e]):i=null==this.mergeMode?[e,e.slice()]:[e],this.returnState?null==this.mergeMode?i.concat(s).concat(s.slice()):[e].concat(s).concat(s.slice()):d.wL(i)}apply(t,e){let i=null==e?null:e.initialState,s=null==e?null:e.constants;null==e&&(e={});const n=tt(t,i,s,this.numConstants);if(t=n.inputs,i=n.initialState,s=n.constants,Array.isArray(t)&&(i=t.slice(1),t=t[0]),(null==i||0===i.length)&&null==s)return super.apply(t,e);const r=[],a=[];if(null!=i){const t=i.length;if(t%2>0)throw new c.Qp("When passing `initialState` to a Bidrectional RNN, the state should be an Array containing the states of the underlying RNNs.");e.initialState=i,r.push(...i);const s=i.map((t=>new o.eO({shape:t.shape})));this.forwardLayer.stateSpec=s.slice(0,t/2),this.backwardLayer.stateSpec=s.slice(t/2),a.push(...s)}if(null!=s)throw new c.EH("Support for constants in Bidirectional layers is not implemented yet.");const l=r[0]instanceof o.Ar;for(const t of r)if(t instanceof o.Ar!==l)throw new c.Qp("The initial state of a Bidirectional layer cannot be specified as a mix of symbolic and non-symbolic tensors");if(l){const i=[t].concat(r),s=this.inputSpec.concat(a),n=this.inputSpec;this.inputSpec=s;const l=super.apply(i,e);return this.inputSpec=n,l}return super.apply(t,e)}call(t,e){return(0,s.DZQ)((()=>{const i=e.initialState;let n,r,a,l;if(null==i)n=this.forwardLayer.call(t,e),r=this.backwardLayer.call(t,e);else{const s=i.slice(0,i.length/2),a=i.slice(i.length/2);n=this.forwardLayer.call(t,Object.assign(e,{initialState:s})),r=this.backwardLayer.call(t,Object.assign(e,{initialState:a}))}return this.returnState&&(Array.isArray(n)&&(a=n.slice(1).concat(r.slice(1))),n=n[0],r=r[0]),this.returnSequences&&(r=s.BEg(r,1)),"concat"===this.mergeMode?l=w.u1([n,r]):"sum"===this.mergeMode?l=s.WQq(n,r):"ave"===this.mergeMode?l=s.lKK(.5,s.WQq(n,r)):"mul"===this.mergeMode?l=s.lKK(n,r):null==this.mergeMode&&(l=[n,r]),this.returnState?null==this.mergeMode?l.concat(a):[l].concat(a):l}))}resetStates(t){this.forwardLayer.resetStates(),this.backwardLayer.resetStates()}build(t){(0,Q.IU)(this.forwardLayer.name,(()=>{this.forwardLayer.build(t)})),(0,Q.IU)(this.backwardLayer.name,(()=>{this.backwardLayer.build(t)})),this.built=!0}computeMask(t,e){let i;if(Array.isArray(e)&&(e=e[0]),i=this.returnSequences?null==this.mergeMode?[e,e]:e:null==this.mergeMode?[null,null]:null,this.returnState){const t=this.forwardLayer.states.map((t=>null));return Array.isArray(i)?i.concat(t).concat(t):[i].concat(t).concat(t)}return i}get trainableWeights(){return this.forwardLayer.trainableWeights.concat(this.backwardLayer.trainableWeights)}get nonTrainableWeights(){return this.forwardLayer.nonTrainableWeights.concat(this.backwardLayer.nonTrainableWeights)}setFastWeightInitDuringBuild(t){super.setFastWeightInitDuringBuild(t),null!=this.forwardLayer&&this.forwardLayer.setFastWeightInitDuringBuild(t),null!=this.backwardLayer&&this.backwardLayer.setFastWeightInitDuringBuild(t)}getConfig(){const t={mergeMode:this.mergeMode},e=super.getConfig();return Object.assign(t,e),t}static fromConfig(t,e){const i=(0,p.i)(e.layer);if(delete e.layer,null!=e.numConstants)throw new c.EH("Deserialization of a Bidirectional layer with numConstants present is not supported yet.");const s=e;return s.layer=i,new t(s)}}oe.className="Bidirectional",s.JFn.registerClass(oe);class he extends o.Wd{constructor(t){super(t),this.scale=t.scale,t.offset?this.offset=t.offset:this.offset=0}getConfig(){const t={scale:this.scale,offset:this.offset},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,s.DZQ)((()=>("float32"!==(t=(0,f.un)(t)).dtype&&(t=w.wg(t,"float32")),(0,s.WQq)((0,s.lKK)(t,this.scale),this.offset))))}}he.className="Rescaling",s.JFn.registerClass(he);const{resizeBilinear:ue,cropAndResize:ce}=s.image;class pe extends o.Wd{constructor(t){super(t),this.height=t.height,this.width=t.width}centerCrop(t,e,i,n,r,a,l,o){return(0,s.DZQ)((()=>{let h,u=!1;const c=[e/a,i/l,(n+e)/a,(r+i)/l],p=[];3===t.rank?(u=!0,h=(0,s.t$z)([t])):h=t;for(let t=0;t<h.shape[0];t++)p.push(c);const d=(0,s.OEK)(p,[p.length,4]),g=(0,s.y17)(0,p.length,1,"int32"),m=ce(h,d,g,[n,r],"nearest");return u?w.wg((0,f.un)((0,s.K$i)(m)),o):w.wg(m,o)}))}upsize(t,e,i,n){return(0,s.DZQ)((()=>{const s=ue(t,[e,i]);return w.wg(s,n)}))}call(t,e){return(0,s.DZQ)((()=>{const e=(0,f.un)(t),i=e.dtype,s=e.shape,n=s[s.length-3],r=s[s.length-2];let a=0;n!==this.height&&(a=Math.floor((n-this.height)/2));let l=0;return r!==this.width&&(l=Math.floor((r-this.width)/2),0===l&&(l=1)),a>=0&&l>=0?this.centerCrop(e,a,l,this.height,this.width,n,r,i):this.upsize(t,this.height,this.width,i)}))}getConfig(){const t={height:this.height,width:this.width},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){const e=(t=(0,f.U$)(t)).length-3,i=t.length-2;return t[e]=this.height,t[i]=this.width,t}}pe.className="CenterCrop",s.JFn.registerClass(pe);class de extends o.Wd{constructor(t){super(t),this.numTokens=t.numTokens,t.outputMode?this.outputMode=t.outputMode:this.outputMode="multiHot"}getConfig(){const t={numTokens:this.numTokens,outputMode:this.outputMode},e=super.getConfig();return Object.assign(t,e),t}computeOutputShape(t){return null==(t=(0,f.U$)(t))?[this.numTokens]:"oneHot"===this.outputMode&&1!==t[t.length-1]?(t.push(this.numTokens),t):(t[t.length-1]=this.numTokens,t)}call(t,e){return(0,s.DZQ)((()=>{let i;if("int32"!==(t=(0,f.un)(t)).dtype&&(t=w.wg(t,"int32")),void 0!==e.countWeights){if("count"!==this.outputMode)throw new c.Qp(`countWeights is not used when outputMode !== count.\n              Received countWeights=${e.countWeights}`);i=(0,f.un)(e.countWeights)}const n=(0,s.T9B)(t),r=(0,s.jkA)(t),a=(0,s.rhj)(this.numTokens,n).bufferSync().get(0),l=(0,s.DQN)(r,0).bufferSync().get(0);if(!a||!l)throw new c.Qp(`Input values must be between 0 < values <= numTokens with numTokens=${this.numTokens}`);return function(t,e,i,n){let r=(0,f.un)(t);if("int32"!==r.dtype&&(r=w.wg(r,"int32")),"int"===e)return r;const a=r.shape;if(0===r.rank&&(r=(0,s.UG6)(r,-1)),"oneHot"===e&&1!==r.shape[r.shape.length-1]&&(r=(0,s.UG6)(r,-1)),r.rank>2)throw new c.Qp(`When outputMode is not int, maximum output rank is 2 Received outputMode ${e} and input shape ${a} which would result in output rank ${r.rank}.`);const l=["multiHot","oneHot"].includes(e),o=r;let h;if(h=void 0!==n&&"count"===e?(0,s.aOp)(o,n,i,l):(0,s.aOp)(o,[],i,l),"tfIdf"!==e)return h;if(n)return(0,s.lKK)(h,n);throw new c.Qp("When outputMode is 'tfIdf', weights must be provided.")}(t,this.outputMode,this.numTokens,i)}))}}de.className="CategoryEncoding",s.JFn.registerClass(de);const ge=new Set(["bilinear","nearest"]);class fe extends o.Wd{constructor(t){if(super(t),this.height=t.height,this.width=t.width,t.interpolation){if(!ge.has(t.interpolation))throw new c.Qp(`Invalid interpolation parameter: ${t.interpolation} is not implemented`);this.interpolation=t.interpolation}else this.interpolation="bilinear";this.cropToAspectRatio=Boolean(t.cropToAspectRatio)}computeOutputShape(t){const e=(t=(0,f.U$)(t))[2];return[this.height,this.width,e]}getConfig(){const t={height:this.height,width:this.width,interpolation:this.interpolation,cropToAspectRatio:this.cropToAspectRatio},e=super.getConfig();return Object.assign(t,e),t}call(t,e){return(0,s.DZQ)((()=>{const e=[this.height,this.width];if("bilinear"===this.interpolation)return s.image.resizeBilinear(t,e,!this.cropToAspectRatio);if("nearest"===this.interpolation)return s.image.resizeNearestNeighbor(t,e,!this.cropToAspectRatio);throw new Error(`Interpolation is ${this.interpolation} but only ${[...ge]} are supported`)}))}}fe.className="Resizing",s.JFn.registerClass(fe),i(37937),i(49044)},1303:(t,e,i)=>{i.d(e,{H4:()=>c,sN:()=>d,Fe:()=>R,zo:()=>D});var s=i(36115),n=i(47661),r=i(39459),a=i(15841);const l=["fanIn","fanOut","fanAvg"],o=["normal","uniform","truncatedNormal"];var h=i(44813),u=i(73072);class c extends s.JFn.Serializable{fromConfigUsesCustomObjects(){return!1}getConfig(){return{}}}class p extends c{apply(t,e){return(0,s.Ul9)(t,e)}}p.className="Zeros",s.JFn.registerClass(p);class d extends c{apply(t,e){return(0,s.SaS)(t,e)}}d.className="Ones",s.JFn.registerClass(d);class g extends c{constructor(t){if(super(),"object"!=typeof t)throw new a.Qp(`Expected argument of type ConstantConfig but got ${t}`);if(void 0===t.value)throw new a.Qp(`config must have value set but got ${t}`);this.value=t.value}apply(t,e){return(0,s.DZQ)((()=>(0,s.lKK)((0,s.d_2)(this.value),(0,s.SaS)(t,e))))}getConfig(){return{value:this.value}}}g.className="Constant",s.JFn.registerClass(g);class f extends c{constructor(t){super(),this.DEFAULT_MINVAL=-.05,this.DEFAULT_MAXVAL=.05,this.minval=t.minval||this.DEFAULT_MINVAL,this.maxval=t.maxval||this.DEFAULT_MAXVAL,this.seed=t.seed}apply(t,e){return(0,s.YeY)(t,this.minval,this.maxval,e,this.seed)}getConfig(){return{minval:this.minval,maxval:this.maxval,seed:this.seed}}}f.className="RandomUniform",s.JFn.registerClass(f);class m extends c{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.EH(`randomNormal does not support dType ${e}.`);return n.FE(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}m.className="RandomNormal",s.JFn.registerClass(m);class b extends c{constructor(t){super(),this.DEFAULT_MEAN=0,this.DEFAULT_STDDEV=.05,this.mean=t.mean||this.DEFAULT_MEAN,this.stddev=t.stddev||this.DEFAULT_STDDEV,this.seed=t.seed}apply(t,e){if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.EH(`truncatedNormal does not support dType ${e}.`);return(0,s.efE)(t,this.mean,this.stddev,e,this.seed)}getConfig(){return{mean:this.mean,stddev:this.stddev,seed:this.seed}}}b.className="TruncatedNormal",s.JFn.registerClass(b);class y extends c{constructor(t){super(),this.gain=null!=t.gain?t.gain:1}apply(t,e){return(0,s.DZQ)((()=>{if(2!==t.length||t[0]!==t[1])throw new a.Qp("Identity matrix initializer can only be used for 2D square matrices.");return(0,s.lKK)(this.gain,(0,s.y5U)(t[0]))}))}getConfig(){return{gain:this.gain}}}y.className="Identity",s.JFn.registerClass(y);class w extends c{constructor(t){if(super(),t.scale<0)throw new a.Qp(`scale must be a positive float. Got: ${t.scale}`);var e;this.scale=null==t.scale?1:t.scale,this.mode=null==t.mode?"fanIn":t.mode,e=this.mode,(0,h.E6)(l,"FanMode",e),this.distribution=null==t.distribution?"normal":t.distribution,function(t){(0,h.E6)(o,"Distribution",t)}(this.distribution),this.seed=t.seed}apply(t,e){const i=function(t,e="channelsLast"){let i,s;if((0,r.uM)(e),2===t.length)i=t[0],s=t[1];else if(-1!==[3,4,5].indexOf(t.length)){if("channelsFirst"===e){const e=(0,u.no)(t,2);i=t[1]*e,s=t[0]*e}else if("channelsLast"===e){const e=(0,u.no)(t,0,t.length-2);i=t[t.length-2]*e,s=t[t.length-1]*e}}else{const e=(0,u.no)(t);i=Math.sqrt(e),s=Math.sqrt(e)}return[i,s]}(t),n=i[0],l=i[1];let o=this.scale;if("fanIn"===this.mode?o/=Math.max(1,n):"fanOut"===this.mode?o/=Math.max(1,l):o/=Math.max(1,(n+l)/2),"normal"===this.distribution){const i=Math.sqrt(o);if("float32"!==(e=e||"float32")&&"int32"!==e)throw new a.EH(`${this.getClassName()} does not support dType ${e}.`);return(0,s.efE)(t,0,i,e,this.seed)}{const i=Math.sqrt(3*o);return(0,s.YeY)(t,-i,i,e,this.seed)}}getConfig(){return{scale:this.scale,mode:this.mode,distribution:this.distribution,seed:this.seed}}}w.className="VarianceScaling",s.JFn.registerClass(w);class z extends w{constructor(t){super({scale:1,mode:"fanAvg",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}z.className="GlorotUniform",s.JFn.registerClass(z);class S extends w{constructor(t){super({scale:1,mode:"fanAvg",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}S.className="GlorotNormal",s.JFn.registerClass(S);class k extends w{constructor(t){super({scale:2,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}k.className="HeNormal",s.JFn.registerClass(k);class C extends w{constructor(t){super({scale:2,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}C.className="HeUniform",s.JFn.registerClass(C);class A extends w{constructor(t){super({scale:1,mode:"fanIn",distribution:"normal",seed:null==t?null:t.seed})}getClassName(){return w.className}}A.className="LeCunNormal",s.JFn.registerClass(A);class v extends w{constructor(t){super({scale:1,mode:"fanIn",distribution:"uniform",seed:null==t?null:t.seed})}getClassName(){return w.className}}v.className="LeCunUniform",s.JFn.registerClass(v);class I extends c{constructor(t){if(super(),this.DEFAULT_GAIN=1,this.gain=null==t.gain?this.DEFAULT_GAIN:t.gain,this.seed=t.seed,null!=this.seed)throw new a.EH("Random seed is not implemented for Orthogonal Initializer yet.")}apply(t,e){return(0,s.DZQ)((()=>{if(t.length<2)throw new a.EH("Shape must be at least 2D.");t[0]*t[1]>2e3&&console.warn(`Orthogonal initializer is being called on a matrix with more than 2000 (${t[0]*t[1]}) elements: Slowness may result.`);const e=t[0]>t[1]?[t[1],t[0]]:t,i=n.FE(e,0,1,"float32");let r=s.mPL.gramSchmidt(i);return t[0]>t[1]&&(r=(0,s.mgz)(r)),(0,s.lKK)(this.gain,r)}))}getConfig(){return{gain:this.gain,seed:this.seed}}}I.className="Orthogonal",s.JFn.registerClass(I);const N={constant:"Constant",glorotNormal:"GlorotNormal",glorotUniform:"GlorotUniform",heNormal:"HeNormal",heUniform:"HeUniform",identity:"Identity",leCunNormal:"LeCunNormal",leCunUniform:"LeCunUniform",ones:"Ones",orthogonal:"Orthogonal",randomNormal:"RandomNormal",randomUniform:"RandomUniform",truncatedNormal:"TruncatedNormal",varianceScaling:"VarianceScaling",zeros:"Zeros"};function F(t,e={}){return(0,h.Xv)(t,s.JFn.SerializationMap.getMap().classNameMap,e,"initializer")}function D(t){return(0,h.k4)(t)}function R(t){if("string"==typeof t){const e=t in N?N[t]:t;if("GlorotNormal"===e)return new S;if("GlorotUniform"===e)return new z;if("HeNormal"===e)return new k;if("HeUniform"===e)return new C;if("LeCunNormal"===e)return new A;if("LeCunUniform"===e)return new v;{const t={};return t.className=e,t.config={},F(t)}}return t instanceof c?t:F(t)}},84604:(t,e,i)=>{i.d(e,{EY:()=>r,To:()=>s,bb:()=>a,ft:()=>n,r$:()=>l});const s=["channelsFirst","channelsLast"],n=["nearest","bilinear"],r=["valid","same","causal"],a=["max","avg"],l=["sum","mul","concat","ave"]},55795:(t,e,i)=>{i.d(e,{i:()=>r});var s=i(36115),n=i(44813);function r(t,e={},i=!1){return(0,n.Xv)(t,s.JFn.SerializationMap.getMap().classNameMap,e,"layer",i)}},96681:(t,e,i)=>{i.d(e,{I:()=>n,i:()=>r});var s=i(36115);async function n(t){if(null==t)return;const e=[],i=[],n=[];for(const s in t){const r=t[s];if("number"!=typeof r){const t=r;e.push(t.data()),i.push(s),n.push(t)}}if(e.length>0){const r=await Promise.all(e);for(let e=0;e<r.length;++e)t[i[e]]=r[e][0];(0,s.ASo)(n)}}function r(t){if(null!=t)for(const e in t){const i=t[e];"number"!=typeof i&&i.dispose()}}},48981:(t,e,i)=>{i.d(e,{Jc:()=>d,Jt:()=>m,Ol:()=>c,Pg:()=>g,SO:()=>f,VW:()=>u,Xv:()=>h,Yq:()=>l,bt:()=>o,qp:()=>p});var s=i(36115),n=i(6090),r=i(47661),a=i(15841);function l(t,e){return(0,s.DZQ)((()=>{"float32"!==t.dtype&&(t=s.wgE(t,"float32"));const i=s.czq(r.Ew(t),e,!0),a=s.GSj(i.shape,(0,n.Ni)()),l=s.RZD(s.PhQ(i,a));return s.y4m(t,l)}))}function o(t,e){return(0,s.DZQ)((()=>s.i2o(r.Ew(s.jbE(e,t)),-1)))}function h(t,e){return(0,s.DZQ)((()=>s.i2o(s.tnl(s.jbE(e,t)),-1)))}function u(t,e){return(0,s.DZQ)((()=>{const i=s.jbE(t,e),r=s.zQh(s.tnl(t),(0,n.Ni)(),Number.MAX_VALUE),a=s.tnl(s.y4m(i,r));return s.lKK(100,s.i2o(a,-1))}))}function c(t,e,i=!1){return(0,s.DZQ)((()=>{if(i)e=s.Vs9(e);else{const t=s.czq(e,e.shape.length-1,!0);e=s.y4m(e,t)}return e=s.zQh(e,(0,n.Ni)(),1-(0,n.Ni)()),s.HZy(s.czq(s.lKK(s.wgE(t,"float32"),s.Rm2(e)),e.shape.length-1))}))}function p(t,e,i=!1){return(0,s.DZQ)((()=>{const a=s.wgE(s.RIf(r.Bq(t)),"int32"),l=(e=s.zQh(e,(0,n.Ni)(),1-(0,n.Ni)())).shape;return c(s.tQQ(s.Mw0(a,l[l.length-1]),l),e,i)}))}function d(t,e){return(0,s.DZQ)((()=>{let i;return i=s.zQh(e,(0,n.Ni)(),1-(0,n.Ni)()),i=s.Rm2(s.y4m(i,s.jbE(1,i))),s.i2o(function(t,e){if(!s.ZSL.arraysEqual(t.shape,e.shape))throw new a.Qp(`logits and labels must have the same shape, but got shapes ${JSON.stringify(t.shape)} and ${JSON.stringify(e.shape)}`);return(0,s.DZQ)((()=>{const i=s.VVh(e),n=s.HZy(s.tnl(e));return s.WQq(s.jbE(i,s.lKK(e,t)),s.Kko(s.oNF(n)))}))}(t,i),-1)}))}function g(t,e){return(0,s.DZQ)((()=>{const i=l(t,-1),n=l(e,-1),r=s.lKK(i,n);return s.HZy(s.czq(r,-1))}))}const f={meanSquaredError:o,meanAbsoluteError:h,meanAbsolutePercentageError:u,meanSquaredLogarithmicError:function(t,e){return(0,s.DZQ)((()=>{const i=s.zQh(e,(0,n.Ni)(),Number.MAX_VALUE),a=s.Rm2(s.WQq(1,i)),l=s.zQh(t,(0,n.Ni)(),Number.MAX_VALUE),o=s.Rm2(s.WQq(1,l));return s.i2o(r.Ew(s.jbE(a,o)),-1)}))},squaredHinge:function(t,e){return(0,s.DZQ)((()=>{const i=s.PhQ(0,s.jbE(1,s.lKK(t,e)));return s.i2o(r.Ew(i),-1)}))},hinge:function(t,e){return(0,s.DZQ)((()=>{const i=s.PhQ(0,s.jbE(1,s.lKK(t,e)));return s.i2o(i,-1)}))},categoricalHinge:function(t,e){return(0,s.DZQ)((()=>{const i=s.czq(s.lKK(t,e),-1),n=s.T9B(s.lKK(s.jbE(1,t),e),-1);return s.PhQ(0,s.WQq(1,s.jbE(n,i)))}))},logcosh:function(t,e){return(0,s.DZQ)((()=>{const i=Math.log(2),n=s.jbE(e,t),r=s.jbE(s.WQq(n,s.lw0(s.lKK(-2,n))),i);return s.i2o(r,-1)}))},categoricalCrossentropy:c,sparseCategoricalCrossentropy:p,binaryCrossentropy:d,kullbackLeiblerDivergence:function(t,e){return(0,s.DZQ)((()=>{const i=s.zQh(t,(0,n.Ni)(),1),r=s.zQh(e,(0,n.Ni)(),1);return s.czq(s.lKK(t,s.Rm2(s.y4m(i,r))),-1)}))},poisson:function(t,e){return(0,s.DZQ)((()=>{const i=s.Rm2(s.WQq((0,n.Ni)(),e));return s.i2o(s.jbE(e,s.lKK(t,i)),-1)}))},cosineProximity:g};function m(t){if("string"==typeof t){if(t in f)return f[t];let e=`Unknown loss ${t}`;throw t.toLowerCase().includes("softmaxcrossentropy")&&(e=`Unknown loss ${t}. Use "categoricalCrossentropy" as the string name for tf.losses.softmaxCrossEntropy`),new a.Qp(e)}return t}},37937:(t,e,i)=>{i.d(e,{GT:()=>C,Jc:()=>u,Jt:()=>k,Ol:()=>y,qp:()=>z,qv:()=>c,xM:()=>o,yP:()=>h});var s=i(36115),n=i(47661),r=i(15841),a=i(48981),l=i(44813);function o(t,e){return(0,s.DZQ)((()=>{const i=s.lKK(.5,s.P61(e)),r=n.wg(s.rhj(e,i),t.dtype);return s.i2o(s.LCg(t,r),-1)}))}function h(t,e){return(0,s.DZQ)((()=>n.wg(s.LCg(s.FLi(t,-1),s.FLi(e,-1)),"float32")))}function u(t,e){return(0,a.Jc)(t,e)}function c(t,e){return t.rank===e.rank&&(t=s.r2V(t,[t.rank-1])),(e=s.FLi(e,-1)).dtype!==t.dtype&&(e=s.wgE(e,t.dtype)),s.wgE(s.LCg(t,e),"float32")}const p=a.bt,d=a.bt,g=a.Xv,f=a.Xv,m=a.VW,b=a.VW,y=a.Ol,w=a.Pg,z=a.qp,S={binaryAccuracy:o,categoricalAccuracy:h,precision:function(t,e){return(0,s.DZQ)((()=>{const i=function(t,e){return(0,s.DZQ)((()=>s.wgE(s.czq(s.n76(s.LCg(t,1),s.LCg(e,1))),"float32")))}(t,e),n=function(t,e){return(0,s.DZQ)((()=>s.wgE(s.czq(s.n76(s.LCg(t,0),s.LCg(e,1))),"float32")))}(t,e),r=s.WQq(i,n);return s.wgE(s._M9(s.rhj(r,0),s.y4m(i,r),0),"float32")}))},categoricalCrossentropy:y,sparseCategoricalCrossentropy:z,mse:p,MSE:d,mae:g,MAE:f,mape:m,MAPE:b,cosine:w};function k(t){if("string"==typeof t&&t in S)return S[t];if("string"!=typeof t&&null!=t)return t;throw new r.Qp(`Unknown metric ${t}`)}function C(t){if(l.vA(null!==t,`Unknown LossOrMetricFn ${t}`),"string"==typeof t)return t;{let e;for(const i of Object.keys(a.SO))if(a.SO[i]===t){e=i;break}if(void 0!==e)return e;for(const i of Object.keys(S))if(S[i]===t){e=i;break}return void 0!==e?e:t.name}}},27430:(t,e,i)=>{i.d(e,{K:()=>a});var s=i(36115),n=i(6090),r=i(15841);function a(t){const e={Adagrad:()=>s.BaG.adagrad(.01),Adadelta:()=>s.BaG.adadelta(1,.95,(0,n.Ni)()),Adam:()=>s.BaG.adam(.001,.9,.999,(0,n.Ni)()),Adamax:()=>s.BaG.adamax(.002,.9,.999,(0,n.Ni)(),0),RMSProp:()=>s.BaG.rmsprop(.001,.9,0,(0,n.Ni)()),SGD:()=>s.BaG.sgd(.01)};if(e.adagrad=e.Adagrad,e.adadelta=e.Adadelta,e.adam=e.Adam,e.adamax=e.Adamax,e.rmsprop=e.RMSProp,e.sgd=e.SGD,t in e)return e[t]();throw new r.Qp(`Unknown Optimizer ${t}`)}},94551:(t,e,i)=>{i.d(e,{Yl:()=>n});const s=1048576;function n(t,e,i=!1){if(null==t||"object"!=typeof t||Object.getPrototypeOf(t)!==Object.prototype||!r(t))throw new Error("User-defined metadata is expected to be a JSON object, but is not.");if(i){const i=JSON.stringify(t);i.length>s&&console.warn(`User-defined metadata of model "${e}" is too large in size (length=${i.length} when serialized). It is not recommended to store such large objects in user-defined metadata. Please make sure its serialized length is <= ${s}.`)}}function r(t){if(null===t)return!0;if("object"==typeof t){if(Object.getPrototypeOf(t)===Object.prototype){const e=Object.keys(t);for(const i of e){if("string"!=typeof i)return!1;if(!r(t[i]))return!1}return!0}if(Array.isArray(t)){for(const e of t)if(!r(e))return!1;return!0}return!1}{const e=typeof t;return"string"===e||"number"===e||"boolean"===e}}},26235:(t,e,i)=>{i.d(e,{Q:()=>s});class s{constructor(t){this.maxEntries=t||100,this.cache=new Map}get(t){let e;return this.cache.has(t)&&(e=this.cache.get(t),this.cache.delete(t),this.cache.set(t,e)),e}put(t,e){if(this.cache.has(t))this.cache.delete(t);else if(this.cache.size>=this.maxEntries){const t=this.cache.keys().next().value;this.cache.delete(t)}this.cache.set(t,e)}getMaxEntries(){return this.maxEntries}setMaxEntries(t){if(t<0)throw new Error(`The maxEntries of LRU caches must be at least 0, but got ${t}.`);if(this.maxEntries>t)for(let e=0;e<this.maxEntries-t;e++){const t=this.cache.keys().next().value;this.cache.delete(t)}this.maxEntries=t}}},44813:(t,e,i)=>{i.d(e,{Am:()=>b,Cb:()=>c,Cd:()=>A,E6:()=>w,HP:()=>z,U9:()=>l,Xv:()=>f,ZF:()=>y,fD:()=>r,k4:()=>d,oo:()=>S,sg:()=>C,st:()=>h,th:()=>m,uc:()=>u,vA:()=>a,wL:()=>o});var s=i(36115),n=i(15841);function r(t,e){if(Array.isArray(t)){let i=[];for(let s=0;s<e;s++)i=i.concat(t);return i}{const i=new Array(e);return i.fill(t),i}}function a(t,e){if(!t)throw new n.pf(e)}function l(t,e){let i=0;for(const s of t)s===e&&i++;return i}function o(t){return 1===t.length?t[0]:t}function h(t){return Array.isArray(t)?t:[t]}function u(t){const e=t.replace(/(.)([A-Z][a-z0-9]+)/g,"$1_$2").replace(/([a-z])([A-Z])/g,"$1_$2").toLowerCase();return"_"!==e[0]?e:"private"+e}function c(t){return t.length<=1||-1===t.indexOf("_")?t:t.replace(/[_]+(\w|$)/g,((t,e)=>e.toUpperCase()))}let p={};function d(t){if(null==t)return null;const e={};return e.className=t.getClassName(),e.config=t.getConfig(),e}function g(t){if(null!=t&&"object"==typeof t)if(Array.isArray(t))t.forEach((t=>g(t)));else{const e=Object.keys(t);for(const i of e){const e=t[i];null!=e&&"object"==typeof e&&(Array.isArray(e)||"ndarray"!==e.type||"number"!=typeof e.value?g(e):t[i]=e.value)}}}function f(t,e={},i={},s="object",r=!1){if("string"==typeof t){const r=t;let a;if(r in i)a=i[r];else if(r in p)a=p[r];else if(a=e[r],null==a)throw new n.Qp(`Unknown ${s}: ${t}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);return a}{const a=t;if(null==a.className||null==a.config)throw new n.Qp(`${s}: Improper config format: ${JSON.stringify(a)}.\n'className' and 'config' must set.`);const l=a.className;let o,h;if(l in i?[o,h]=i[l]:l in p?[o,h]=p.className:l in e&&([o,h]=e[l]),null==o)throw new n.Qp(`Unknown ${s}: ${l}. This may be due to one of the following reasons:\n1. The ${s} is defined in Python, in which case it needs to be ported to TensorFlow.js or your JavaScript code.\n2. The custom ${s} is defined in JavaScript, but is not registered properly with tf.serialization.registerClass().`);if(null!=h){const t={};for(const e of Object.keys(p))t[e]=p[e];for(const e of Object.keys(i))t[e]=i[e];a.config.customObjects=t;const e=Object.assign({},p);for(const t of Object.keys(i))p[t]=i[t];g(a.config);const s=h(o,a.config,i,r);return p=Object.assign({},e),s}{const t=Object.assign({},p);for(const t of Object.keys(i))p[t]=i[t];const e=new o(a.config);return p=Object.assign({},t),e}}}function m(t,e){return-1*function(t,e){return t<e?-1:t>e?1:0}(t,e)}function b(t){if(null==t)return t;const e=[];for(const i of t)-1===e.indexOf(i)&&e.push(i);return e}function y(t){if(null==t)throw new n.Qp(`Invalid value in obj: ${JSON.stringify(t)}`);for(const e in t)if(t.hasOwnProperty(e))return!1;return!0}function w(t,e,i){if(null!=i&&t.indexOf(i)<0)throw new n.Qp(`${i} is not a valid ${e}.  Valid values are ${t} or null/undefined.`)}function z(t,e,i=0,s=1/0){return a(i>=0),a(s>=i),Array.isArray(t)&&t.length>=i&&t.length<=s&&t.every((t=>typeof t===e))}function S(t,e){Array.isArray(t)?(s.ZSL.assert(t.length>0,(()=>`${e} is unexpectedly an empty array.`)),t.forEach(((t,i)=>S(t,`element ${i+1} of ${e}`)))):s.ZSL.assert(Number.isInteger(t)&&t>0,(()=>`Expected ${e} to be a positive integer, but got ${k(t)}.`))}function k(t){return null===t?"null":Array.isArray(t)?"["+t.map((t=>k(t))).join(",")+"]":"string"==typeof t?`"${t}"`:`${t}`}function C(t,e,i){let n,r=null!=i?i():s.ZSL.now();return(...a)=>{const l=null!=i?i():s.ZSL.now();return l-r<e||(r=l,n=t(...a)),n}}function A(t){return"relu"===t?"relu":"linear"===t?"linear":"elu"===t?"elu":null}},9917:(t,e,i)=>{i.d(e,{o:()=>n});var s=i(54390);function n(t,e,i,n=console.log){const o=function(t){let e=!0;const i=[],s=[];for(const e in t.nodesByDepth)i.push(t.nodesByDepth[e]);for(const t of i){if(t.length>1||1===t.length&&t[0].inboundLayers.length>1){e=!1;break}s.push(...t)}if(e)for(const i of t.layers){let t=!1;for(const n of i.inboundNodes)if(-1!==s.indexOf(n)){if(t){e=!1;break}t=!0}if(!e)break}return e}(t),h=["Layer (type)","Input Shape","Output shape","Param #"];let u;if(o?(e=e||90,i=i||[.32,.61,.89,1]):(e=e||115,i=i||[.24,.48,.7,.8,1]),i[i.length-1]<=1&&(i=i.map((t=>Math.floor(e*t)))),!o){h.push("Receives inputs"),u=[];for(const e in t.nodesByDepth)u.push(...t.nodesByDepth[e])}n("_".repeat(e)),r(h,i,n),n("=".repeat(e));const c=t.layers;for(let t=0;t<c.length;++t)o?a(c[t],i,n):l(c[t],i,u,n),n((t===c.length-1?"=":"_").repeat(e));t.checkTrainableWeightsConsistency();const p=function(t){let e;return e=null!=t.collectedTrainableWeights?(0,s.Y)(t.collectedTrainableWeights):(0,s.Y)(t.trainableWeights),e}(t),d=(0,s.Y)(t.nonTrainableWeights);n(`Total params: ${p+d}`),n(`Trainable params: ${p}`),n(`Non-trainable params: ${d}`),n("_".repeat(e))}function r(t,e,i=console.log){let s="";for(let i=0;i<t.length;++i)i>0&&(s=s.slice(0,s.length-1)+" "),s+=t[i],s=s.slice(0,e[i]),s+=" ".repeat(e[i]-s.length);i(s)}function a(t,e,i){let s,n;try{n=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(t){n="multiple"}try{s=JSON.stringify(t.outputShape)}catch(t){s="multiple"}r([`${t.name} (${t.getClassName()})`,n,s,t.countParams().toString()],e,i)}function l(t,e,i,s){let n,a;try{a=t.inboundNodes.map((t=>JSON.stringify(t.inputShapes))).join(",")}catch(t){a="multiple"}try{n=JSON.stringify(t.outputShape)}catch(t){n="multiple"}const l=[];for(const e of t.inboundNodes)if(!(null!=i&&i.length>0&&-1===i.indexOf(e)))for(let t=0;t<e.inboundLayers.length;++t){const i=e.inboundLayers[t].name,s=e.nodeIndices[t],n=e.tensorIndices[t];l.push(`${i}[${s}][${n}]`)}const o=t.name,h=t.getClassName(),u=0===l.length?"":l[0];r([`${o} (${h})`,a,n,t.countParams().toString(),u],e,s);for(let t=1;t<l.length;++t)r(["","","","",l[t]],e,s)}},73072:(t,e,i)=>{i.d(e,{Fq:()=>n,T9:()=>l,jk:()=>a,no:()=>r,y1:()=>o});var s=i(15841);function n(t){return t===parseInt(t.toString(),10)}function r(t,e,i){null==e&&(e=0),null==i&&(i=t.length);let s=1;for(let n=e;n<i;++n)s*=t[n];return s}function a(t){if(0===t.length)return Number.NaN;let e=Number.POSITIVE_INFINITY;for(let i=0;i<t.length;i++){const s=t[i];s<e&&(e=s)}return e}function l(t){if(0===t.length)return Number.NaN;let e=Number.NEGATIVE_INFINITY;for(let i=0;i<t.length;i++){const s=t[i];s>e&&(e=s)}return e}function o(t,e){if(e<t)throw new s.Qp(`end (${e}) < begin (${t}) is forbidden.`);const i=[];for(let s=t;s<e;++s)i.push(s);return i}},91928:(t,e,i)=>{i.d(e,{M:()=>a,w:()=>r});var s=i(44813);function n(t,e,i){return("inboundNodes"===t||"outputLayers"===t||"inputLayers"===t)&&0===e&&"string"==typeof i}function r(t,e){if(null===t)return null;if("string"==typeof t)return s.Cb(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const i=[],s=t.length;for(let a=0;a<s;++a){const s=t[a];n(e,a,s)?i.push(s):i.push(r(s,e))}return i}{const e={};for(const i of Object.keys(t)){const n=t[i];if("name"===i&&"string"==typeof n)e[i]=n;else{const t=s.Cb(i);e[t]=r(n,t)}}return e}}function a(t,e){if(null==t)return null;if("string"==typeof t)return s.uc(t);if("number"==typeof t||"boolean"==typeof t)return t;if(t instanceof Array){const i=[],s=t.length;for(let r=0;r<s;++r){const s=t[r];n(e,r,s)?i.push(s):i.push(a(s,e))}return i}{const e={};for(const i of Object.keys(t)){const n=t[i];e[s.uc(i)]="name"!==i&&"className"!==i||"string"!=typeof n?a(n,i):n}return e}}},63057:(t,e,i)=>{i.d(e,{FS:()=>r,TT:()=>n,U$:()=>l,un:()=>a});var s=i(15841);function n(t){return Array.isArray(t)&&Array.isArray(t[0])}function r(t){return 0===t.length?[]:Array.isArray(t[0])?t:[t]}function a(t){let e;if(Array.isArray(t)){if(1!==t.length)throw new s.Qp(`Expected Tensor length to be 1; got ${t.length}`);e=t[0]}else e=t;return e}function l(t){if(Array.isArray(t)&&Array.isArray(t[0])){if(1===t.length)return t[0];throw new s.Qp(`Expected exactly 1 Shape; got ${t.length}`)}return t}},54390:(t,e,i)=>{function s(t){let e=0;for(const i of t)0===i.shape.length?e+=1:e+=i.shape.reduce(((t,e)=>t*e));return e}i.d(e,{Y:()=>s})},71765:(t,e,i)=>{i.d(e,{UM:()=>h,eR:()=>l,ex:()=>o});var s=i(36115),n=i(91686),r=i(39459);i(15841);const a="Variable";class l{constructor(t,e="float32",i=a,l=!0,o=null){this.dtype=null==e?"float32":e,this.shape=t.shape,this.id=(0,n.j)(),i=null==i?a:i,this.originalName=(0,r.BC)(i),this.name=(0,r.Uc)(this.originalName),this.trainable_=l,this.constraint=o,this.val=s.bvq(t,this.trainable_,this.name,this.dtype)}read(){return this.assertNotDisposed(),this.val}write(t){return this.assertNotDisposed(),function(t,e){if(t.shape.toString()!==e.shape.toString())throw new Error("Shape mismatch: "+JSON.stringify(t.shape)+" vs. "+JSON.stringify(e.shape))}(this.val,t),this.val.id!==t.id&&(this.val.assign(t),null!=this.constraint&&this.val.assign(this.constraint.apply(this.val))),this}dispose(){this.assertNotDisposed(),this.val.dispose()}assertNotDisposed(){if(this.val.isDisposed)throw new Error(`LayersVariable ${this.name} is already disposed.`)}get trainable(){return this.trainable_}set trainable(t){this.trainable_=t,this.val.trainable=t}}function o(t){return t.map((t=>t.read()))}function h(t){t.forEach((t=>{t[0].write(t[1])}))}},52700:(t,e,i)=>{i.d(e,{r:()=>s});const s="4.2.0"}}]);